{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  # Import TensorFlow library\n",
        "from tensorflow.keras import layers, models  # Import layers and models from Keras\n",
        "import numpy as np  # Import NumPy library\n",
        "import datetime  # Import datetime for timing\n",
        "import multiprocessing  # Import multiprocessing for parallel processing\n",
        "\n",
        "def download_mnist_images():\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    # Reshape and normalize the training and testing images\n",
        "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "    # Convert labels to categorical (one-hot encoded) format\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def create_part1():\n",
        "    # Define the first part of the model\n",
        "    inputs = tf.keras.Input(shape=(784,))  # Input layer with shape 784\n",
        "    x = layers.Dense(128, activation='relu')(inputs)  # Dense layer with 128 units and ReLU activation\n",
        "    outputs = layers.Dense(64, activation='relu')(x)  # Dense layer with 64 units and ReLU activation\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the model\n",
        "    return model\n",
        "\n",
        "def create_part2():\n",
        "    # Define the second part of the model\n",
        "    inputs = tf.keras.Input(shape=(64,))  # Input layer with shape 64\n",
        "    outputs = layers.Dense(10, activation='softmax')(inputs)  # Output layer with 10 units and softmax activation\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the model\n",
        "    return model\n",
        "\n",
        "def create_combined_model(part1, part2):\n",
        "    # Combine part1 and part2 into a single model\n",
        "    inputs = tf.keras.Input(shape=(784,))  # Input layer with shape 784\n",
        "    x = part1(inputs)  # Pass input through part1\n",
        "    outputs = part2(x)  # Pass the output of part1 through part2\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the combined model\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    # Evaluate the model on the test dataset\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)  # Evaluate and get the loss and accuracy\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train_part1(x_train, y_train, epochs, batch_size, return_dict, event):\n",
        "    # Train part1 of the model\n",
        "    part1 = create_part1()  # Create part1 model\n",
        "    part1.compile(optimizer='adam', loss='mse')  # Compile the model with MSE loss\n",
        "    part1.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=1)  # Train the model\n",
        "    intermediate_output = part1.predict(x_train)  # Get the intermediate output\n",
        "    return_dict['part1'] = part1  # Store the trained part1 model in the shared dictionary\n",
        "    return_dict['intermediate_output'] = intermediate_output  # Store the intermediate output in the shared dictionary\n",
        "    event.set()  # Signal that part1 has finished training\n",
        "\n",
        "def train_part2(y_train, epochs, batch_size, return_dict, event):\n",
        "    # Wait for part1 to finish and the intermediate output to be ready\n",
        "    event.wait()\n",
        "    # Train part2 of the model\n",
        "    intermediate_output = return_dict['intermediate_output']  # Retrieve intermediate output\n",
        "    part2 = create_part2()  # Create part2 model\n",
        "    part2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model with categorical crossentropy loss\n",
        "    part2.fit(intermediate_output, y_train, batch_size=batch_size, epochs=epochs, verbose=1)  # Train the model\n",
        "    return_dict['part2'] = part2  # Store the trained part2 model in the shared dictionary\n",
        "\n",
        "def train_and_evaluate_parallel_parts():\n",
        "    # Load and preprocess the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = download_mnist_images()\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 10\n",
        "    batch_size = 128\n",
        "\n",
        "    # Create a manager for shared dictionary and an event for synchronization\n",
        "    manager = multiprocessing.Manager()\n",
        "    return_dict = manager.dict()  # Shared dictionary to store results from processes\n",
        "    event = multiprocessing.Event()  # Event to signal when part1 has finished training\n",
        "\n",
        "    # Create processes for training part1 and part2\n",
        "    process1 = multiprocessing.Process(target=train_part1, args=(x_train, y_train, epochs, batch_size, return_dict, event))\n",
        "    process1.start()  # Start training part1\n",
        "\n",
        "    # Now start part2 training\n",
        "    process2 = multiprocessing.Process(target=train_part2, args=(y_train, epochs, batch_size, return_dict, event))\n",
        "    process2.start()  # Start training part2\n",
        "\n",
        "    # Wait for both processes to complete\n",
        "    process1.join()\n",
        "    process2.join()\n",
        "\n",
        "    part1 = return_dict['part1']  # Retrieve the trained part1 model\n",
        "    part2 = return_dict['part2']  # Retrieve the trained part2 model\n",
        "\n",
        "    # Combine part1 and part2 for evaluation\n",
        "    combined_model = create_combined_model(part1, part2)\n",
        "\n",
        "    # Evaluate the combined model on the test dataset\n",
        "    intermediate_output_test = part1.predict(x_test)  # Get intermediate output for test data\n",
        "    test_loss, test_acc = evaluate_model(part2, intermediate_output_test, y_test)  # Evaluate part2 with intermediate output\n",
        "    print(f'Test accuracy (parallel training): {test_acc}')\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Measure time for the entire process\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    print(\"Training parts in parallel and evaluating combined model...\")\n",
        "    parallel_model_acc = train_and_evaluate_parallel_parts()  # Train parts in parallel and evaluate the model\n",
        "\n",
        "    # Measure end time for the entire process\n",
        "    end_time = datetime.datetime.now()\n",
        "    # Calculate total time for the entire process\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "    print(f\"Total time for the entire process: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Print the final accuracy\n",
        "    print(f\"Parallel training - Test accuracy: {parallel_model_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "OAouUqWYAnIW",
        "outputId": "bb1b1d8f-494f-4dce-d707-e3e9d5bcc553"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training parts in parallel and evaluating combined model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process Process-5:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-10-f51f773ebbeb>\", line 58, in train_part2\n",
            "    event.wait()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 349, in wait\n",
            "    self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
            "    return self._wait_semaphore.acquire(True, timeout)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f51f773ebbeb>\u001b[0m in \u001b[0;36m<cell line: 104>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training parts in parallel and evaluating combined model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mparallel_model_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_parallel_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train parts in parallel and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Measure end time for the entire process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f51f773ebbeb>\u001b[0m in \u001b[0;36mtrain_and_evaluate_parallel_parts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# Wait for both processes to complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mprocess1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mprocess2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mpart1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'part1'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Retrieve the trained part1 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5_P-DCS44jE",
        "outputId": "f954c18b-6114-422d-ec75-c75569634797"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed mnist_images/image_0.jpg\n",
            "Processed mnist_images/image_1.jpg\n",
            "Processed mnist_images/image_2.jpg\n",
            "Processed mnist_images/image_3.jpg\n",
            "Processed mnist_images/image_4.jpg\n",
            "Processed mnist_images/image_5.jpg\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "import queue\n",
        "import time\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def download_mnist_images():\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    image_dir = 'mnist_images'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    image_paths = []\n",
        "\n",
        "    # Save some images from the dataset\n",
        "    for i in range(6):\n",
        "        image_path = os.path.join(image_dir, f'image_{i}.jpg')\n",
        "        img = Image.fromarray(x_train[i])\n",
        "        img.save(image_path)\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def worker(work_queue, result_queue):\n",
        "    while not work_queue.empty():\n",
        "        try:\n",
        "            image_path = work_queue.get_nowait()\n",
        "            convert_to_grayscale(image_path)\n",
        "            result_queue.put(f\"Processed {image_path}\")\n",
        "        except queue.Empty:\n",
        "            break\n",
        "\n",
        "def convert_to_grayscale(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert image to grayscale\n",
        "    grayscale_path = f\"grayscale_{os.path.basename(image_path)}\"\n",
        "    img.save(grayscale_path)\n",
        "    time.sleep(1)  # Simulate processing time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    image_paths = download_mnist_images()\n",
        "    work_queue = multiprocessing.Queue()\n",
        "    result_queue = multiprocessing.Queue()\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        work_queue.put(image_path)\n",
        "\n",
        "    workers = []\n",
        "    for i in range(3):  # Creating 3 workers\n",
        "        process = multiprocessing.Process(target=worker, args=(work_queue, result_queue))\n",
        "        process.start()\n",
        "        workers.append(process)\n",
        "\n",
        "    for worker in workers:\n",
        "        worker.join()\n",
        "\n",
        "    while not result_queue.empty():\n",
        "        print(result_queue.get())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing\n",
        "import queue\n",
        "import time\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime\n",
        "\n",
        "def download_mnist_images():\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    image_dir = 'mnist_images'\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    image_paths = []\n",
        "\n",
        "    # Save some images from the dataset\n",
        "    for i in range(60):\n",
        "        image_path = os.path.join(image_dir, f'image_{i}.jpg')\n",
        "        img = Image.fromarray(x_train[i])\n",
        "        img.save(image_path)\n",
        "        image_paths.append(image_path)\n",
        "\n",
        "    return image_paths\n",
        "\n",
        "def worker(work_queue, result_queue, worker_id, metrics_queue):\n",
        "    start_time = time.time()\n",
        "    tasks_processed = 0\n",
        "    while not work_queue.empty():\n",
        "        try:\n",
        "            image_path = work_queue.get_nowait()\n",
        "            convert_to_grayscale(image_path)\n",
        "            result_queue.put(f\"Processed {image_path}\")\n",
        "            tasks_processed += 1\n",
        "        except queue.Empty:\n",
        "            break\n",
        "    end_time = time.time()\n",
        "    metrics_queue.put((worker_id, tasks_processed, start_time, end_time))\n",
        "\n",
        "def convert_to_grayscale(image_path):\n",
        "    img = Image.open(image_path).convert('L')  # Convert image to grayscale\n",
        "    grayscale_path = f\"grayscale_{os.path.basename(image_path)}\"\n",
        "    img.save(grayscale_path)\n",
        "    time.sleep(1)  # Simulate processing time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = datetime.datetime.now()\n",
        "    image_paths = download_mnist_images()\n",
        "    work_queue = multiprocessing.Queue()\n",
        "    result_queue = multiprocessing.Queue()\n",
        "    metrics_queue = multiprocessing.Queue()\n",
        "\n",
        "    for image_path in image_paths:\n",
        "        work_queue.put(image_path)\n",
        "\n",
        "    workers = []\n",
        "    for i in range(3):  # Creating 3 workers\n",
        "        process = multiprocessing.Process(target=worker, args=(work_queue, result_queue, i, metrics_queue))\n",
        "        process.start()\n",
        "        workers.append(process)\n",
        "\n",
        "    for worker in workers:\n",
        "        worker.join()\n",
        "\n",
        "    end_time = datetime.datetime.now()\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "\n",
        "    while not result_queue.empty():\n",
        "        print(result_queue.get())\n",
        "\n",
        "    metrics = []\n",
        "    while not metrics_queue.empty():\n",
        "        metrics.append(metrics_queue.get())\n",
        "\n",
        "    # Display metrics\n",
        "    for worker_id, tasks_processed, start, end in metrics:\n",
        "        worker_time = end - start\n",
        "        print(f\"Worker {worker_id} processed {tasks_processed} tasks in {worker_time:.2f} seconds\")\n",
        "\n",
        "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwCYkUM15sAK",
        "outputId": "b7aada20-715e-4f9d-a5cc-99d8f9eca08c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed mnist_images/image_0.jpg\n",
            "Processed mnist_images/image_1.jpg\n",
            "Processed mnist_images/image_2.jpg\n",
            "Processed mnist_images/image_3.jpg\n",
            "Processed mnist_images/image_4.jpg\n",
            "Processed mnist_images/image_5.jpg\n",
            "Processed mnist_images/image_6.jpg\n",
            "Processed mnist_images/image_7.jpg\n",
            "Processed mnist_images/image_8.jpg\n",
            "Processed mnist_images/image_9.jpg\n",
            "Processed mnist_images/image_10.jpg\n",
            "Processed mnist_images/image_11.jpg\n",
            "Processed mnist_images/image_12.jpg\n",
            "Processed mnist_images/image_13.jpg\n",
            "Processed mnist_images/image_14.jpg\n",
            "Processed mnist_images/image_15.jpg\n",
            "Processed mnist_images/image_16.jpg\n",
            "Processed mnist_images/image_17.jpg\n",
            "Processed mnist_images/image_18.jpg\n",
            "Processed mnist_images/image_19.jpg\n",
            "Processed mnist_images/image_20.jpg\n",
            "Processed mnist_images/image_21.jpg\n",
            "Processed mnist_images/image_22.jpg\n",
            "Processed mnist_images/image_23.jpg\n",
            "Processed mnist_images/image_24.jpg\n",
            "Processed mnist_images/image_25.jpg\n",
            "Processed mnist_images/image_26.jpg\n",
            "Processed mnist_images/image_27.jpg\n",
            "Processed mnist_images/image_28.jpg\n",
            "Processed mnist_images/image_29.jpg\n",
            "Processed mnist_images/image_30.jpg\n",
            "Processed mnist_images/image_31.jpg\n",
            "Processed mnist_images/image_32.jpg\n",
            "Processed mnist_images/image_33.jpg\n",
            "Processed mnist_images/image_34.jpg\n",
            "Processed mnist_images/image_35.jpg\n",
            "Processed mnist_images/image_36.jpg\n",
            "Processed mnist_images/image_37.jpg\n",
            "Processed mnist_images/image_38.jpg\n",
            "Processed mnist_images/image_39.jpg\n",
            "Processed mnist_images/image_40.jpg\n",
            "Processed mnist_images/image_41.jpg\n",
            "Processed mnist_images/image_42.jpg\n",
            "Processed mnist_images/image_43.jpg\n",
            "Processed mnist_images/image_44.jpg\n",
            "Processed mnist_images/image_45.jpg\n",
            "Processed mnist_images/image_46.jpg\n",
            "Processed mnist_images/image_47.jpg\n",
            "Processed mnist_images/image_48.jpg\n",
            "Processed mnist_images/image_49.jpg\n",
            "Processed mnist_images/image_50.jpg\n",
            "Processed mnist_images/image_51.jpg\n",
            "Processed mnist_images/image_52.jpg\n",
            "Processed mnist_images/image_53.jpg\n",
            "Processed mnist_images/image_54.jpg\n",
            "Processed mnist_images/image_55.jpg\n",
            "Processed mnist_images/image_56.jpg\n",
            "Processed mnist_images/image_57.jpg\n",
            "Processed mnist_images/image_58.jpg\n",
            "Processed mnist_images/image_59.jpg\n",
            "Worker 0 processed 20 tasks in 20.04 seconds\n",
            "Worker 1 processed 20 tasks in 20.03 seconds\n",
            "Worker 2 processed 20 tasks in 20.04 seconds\n",
            "Total time for processing: 20.40 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def download_mnist_images():\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def create_part1():\n",
        "    inputs = tf.keras.Input(shape=(784,))\n",
        "    x = layers.Dense(128, activation='relu')(inputs)\n",
        "    outputs = layers.Dense(64, activation='relu')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_part2():\n",
        "    inputs = tf.keras.Input(shape=(64,))\n",
        "    outputs = layers.Dense(10, activation='softmax')(inputs)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_combined_model(part1, part2):\n",
        "    inputs = tf.keras.Input(shape=(784,))\n",
        "    x = part1(inputs)\n",
        "    outputs = part2(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = datetime.datetime.now()\n",
        "    (x_train, y_train), (x_test, y_test) = download_mnist_images()\n",
        "\n",
        "    # Create strategy for distributing training\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        part1 = create_part1()\n",
        "        part2 = create_part2()\n",
        "        combined_model = create_combined_model(part1, part2)\n",
        "\n",
        "        # Compile the combined model\n",
        "        combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the combined model\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "\n",
        "    combined_model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "\n",
        "    # Evaluate the combined model\n",
        "    test_loss, test_acc = combined_model.evaluate(x_test, y_test)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "    end_time = datetime.datetime.now()\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "    print(f\"Total time for processing: {total_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqcHusRO63GN",
        "outputId": "0f36194f-5001-416a-e893-b2254e58c048"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 6s 9ms/step - loss: 0.3349 - accuracy: 0.9062\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1361 - accuracy: 0.9610\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0943 - accuracy: 0.9722\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.0731 - accuracy: 0.9779\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0574 - accuracy: 0.9826\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0458 - accuracy: 0.9859\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0366 - accuracy: 0.9888\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0302 - accuracy: 0.9906\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0243 - accuracy: 0.9925\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0198 - accuracy: 0.9941\n",
            "313/313 [==============================] - 2s 6ms/step - loss: 0.0829 - accuracy: 0.9769\n",
            "Test accuracy: 0.9768999814987183\n",
            "Total time for processing: 34.35 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def download_mnist_images():\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def create_part1():\n",
        "    inputs = tf.keras.Input(shape=(784,))\n",
        "    x = layers.Dense(128, activation='relu')(inputs)\n",
        "    outputs = layers.Dense(64, activation='relu')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_part2():\n",
        "    inputs = tf.keras.Input(shape=(64,))\n",
        "    outputs = layers.Dense(10, activation='softmax')(inputs)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_combined_model(part1, part2):\n",
        "    inputs = tf.keras.Input(shape=(784,))\n",
        "    x = part1(inputs)\n",
        "    outputs = part2(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train_and_evaluate_combined_model():\n",
        "    (x_train, y_train), (x_test, y_test) = download_mnist_images()\n",
        "\n",
        "    # Create strategy for distributing training\n",
        "    strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "    with strategy.scope():\n",
        "        part1 = create_part1()\n",
        "        part2 = create_part2()\n",
        "        combined_model = create_combined_model(part1, part2)\n",
        "\n",
        "        # Compile the combined model\n",
        "        combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training the combined model\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = datetime.datetime.now()\n",
        "        combined_model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1)\n",
        "        epoch_end_time = datetime.datetime.now()\n",
        "        epoch_time = (epoch_end_time - epoch_start_time).total_seconds()\n",
        "        epoch_times.append(epoch_time)\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f} seconds')\n",
        "\n",
        "    # Evaluate the combined model\n",
        "    test_loss, test_acc = evaluate_model(combined_model, x_test, y_test)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "    total_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "    return test_acc, total_time\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    print(\"Training and evaluating combined model...\")\n",
        "    combined_model_acc, combined_model_time = train_and_evaluate_combined_model()\n",
        "\n",
        "    end_time = datetime.datetime.now()\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "    print(f\"Total time for the entire process: {total_time:.2f} seconds\")\n",
        "\n",
        "    print(f\"Combined model - Test accuracy: {combined_model_acc}, Training time: {combined_model_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0PSsFGQ9dQ4",
        "outputId": "428a47c9-395d-43f3-f873-e29394df638a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating combined model...\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3287 - accuracy: 0.9089\n",
            "Epoch 1/10 - Time: 3.87 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1335 - accuracy: 0.9609\n",
            "Epoch 2/10 - Time: 3.24 seconds\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0933 - accuracy: 0.9720\n",
            "Epoch 3/10 - Time: 5.96 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0715 - accuracy: 0.9786\n",
            "Epoch 4/10 - Time: 2.84 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0562 - accuracy: 0.9832\n",
            "Epoch 5/10 - Time: 3.21 seconds\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0449 - accuracy: 0.9863\n",
            "Epoch 6/10 - Time: 5.83 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0369 - accuracy: 0.9890\n",
            "Epoch 7/10 - Time: 3.19 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0304 - accuracy: 0.9908\n",
            "Epoch 8/10 - Time: 3.21 seconds\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0250 - accuracy: 0.9921\n",
            "Epoch 9/10 - Time: 3.60 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0213 - accuracy: 0.9937\n",
            "Epoch 10/10 - Time: 3.27 seconds\n",
            "Test accuracy: 0.9764999747276306\n",
            "Total training time: 38.22 seconds\n",
            "Total time for the entire process: 39.94 seconds\n",
            "Combined model - Test accuracy: 0.9764999747276306, Training time: 38.22 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def download_mnist_images():\n",
        "    # Load the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "    # Reshape and normalize the training and testing images\n",
        "    x_train = x_train.reshape(-1, 784).astype('float32') / 255.0\n",
        "    x_test = x_test.reshape(-1, 784).astype('float32') / 255.0\n",
        "    # Convert labels to categorical (one-hot encoded) format\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def create_part1():\n",
        "    inputs = tf.keras.Input(shape=(784,))\n",
        "    x = layers.Dense(128, activation='relu')(inputs)\n",
        "    outputs = layers.Dense(64, activation='relu')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_part2():\n",
        "    inputs = tf.keras.Input(shape=(64,))\n",
        "    outputs = layers.Dense(10, activation='softmax')(inputs)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_combined_model(part1, part2):\n",
        "    inputs = tf.keras.Input(shape=(784,))\n",
        "    x = part1(inputs)\n",
        "    outputs = part2(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train_and_evaluate_combined_model():\n",
        "    # Load and preprocess the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = download_mnist_images()\n",
        "\n",
        "    # Create strategy for distributing training\n",
        "    strategy = tf.distribute.MirroredStrategy()  # Use MirroredStrategy for multi-GPU training\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Create parts of the model and combine them\n",
        "        part1 = create_part1()\n",
        "        part2 = create_part2()\n",
        "        combined_model = create_combined_model(part1, part2)\n",
        "\n",
        "        # Compile the combined model\n",
        "        combined_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "\n",
        "    epoch_times = []  # List to store the time taken for each epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Record start time of the epoch\n",
        "        epoch_start_time = datetime.datetime.now()\n",
        "        # Train the combined model for one epoch\n",
        "        combined_model.fit(x_train, y_train, batch_size=batch_size, epochs=1, verbose=1)\n",
        "        # Record end time of the epoch\n",
        "        epoch_end_time = datetime.datetime.now()\n",
        "        # Calculate the duration of the epoch\n",
        "        epoch_time = (epoch_end_time - epoch_start_time).total_seconds()\n",
        "        epoch_times.append(epoch_time)  # Store the epoch duration\n",
        "        print(f'Epoch {epoch+1}/{epochs} - Time: {epoch_time:.2f} seconds')\n",
        "\n",
        "    # Evaluate the combined model on the test dataset\n",
        "    test_loss, test_acc = evaluate_model(combined_model, x_test, y_test)\n",
        "    print(f'Test accuracy: {test_acc}')\n",
        "\n",
        "    # Calculate total training time\n",
        "    total_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_time:.2f} seconds\")\n",
        "\n",
        "    return test_acc, total_time\n",
        "\n",
        "def train_and_evaluate_parallel_parts():\n",
        "    # Load and preprocess the MNIST dataset\n",
        "    (x_train, y_train), (x_test, y_test) = download_mnist_images()\n",
        "\n",
        "    # Create strategy for distributing training\n",
        "    strategy = tf.distribute.MirroredStrategy()  # Use MirroredStrategy for multi-GPU training\n",
        "\n",
        "    with strategy.scope():\n",
        "        # Create parts of the model\n",
        "        part1 = create_part1()\n",
        "        part2 = create_part2()\n",
        "\n",
        "        # Compile the models separately\n",
        "        part1.compile(optimizer='adam', loss='mse')  # Use MSE to learn good features\n",
        "        part2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Training parameters\n",
        "    batch_size = 128\n",
        "    epochs = 10\n",
        "\n",
        "    # Train part1 to generate good features\n",
        "    part1.fit(x_train, x_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "\n",
        "    # Generate intermediate outputs using part1\n",
        "    intermediate_output = part1.predict(x_train)\n",
        "\n",
        "    # Train part2 using the intermediate outputs\n",
        "    part2.fit(intermediate_output, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "\n",
        "    # Combine part1 and part2 for evaluation\n",
        "    combined_model = create_combined_model(part1, part2)\n",
        "\n",
        "    # Evaluate the combined model on the test dataset\n",
        "    intermediate_output_test = part1.predict(x_test)\n",
        "    test_loss, test_acc = evaluate_model(part2, intermediate_output_test, y_test)\n",
        "    print(f'Test accuracy (parallel training): {test_acc}')\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Measure time for the entire process\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    print(\"Training and evaluating combined model...\")\n",
        "    combined_model_acc, combined_model_time = train_and_evaluate_combined_model()  # Train and evaluate the model\n",
        "\n",
        "    print(\"Training parts in parallel and evaluating combined model...\")\n",
        "    parallel_model_acc = train_and_evaluate_parallel_parts()  # Train parts in parallel and evaluate the model\n",
        "\n",
        "    # Measure end time for the entire process\n",
        "    end_time = datetime.datetime.now()\n",
        "    # Calculate total time for the entire process\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "    print(f\"Total time for the entire process: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Print the final accuracy and training time for both approaches\n",
        "    print(f\"Combined model - Test accuracy: {combined_model_acc}, Training time: {combined_model_time:.2f} seconds\")\n",
        "    print(f\"Parallel training - Test accuracy: {parallel_model_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GwNy5wP3-WNX",
        "outputId": "318b2d0d-8ef1-467d-de2e-910f9a87c0ab"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating combined model...\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 0.3352 - accuracy: 0.9050\n",
            "Epoch 1/10 - Time: 6.38 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.1351 - accuracy: 0.9610\n",
            "Epoch 2/10 - Time: 3.18 seconds\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0937 - accuracy: 0.9728\n",
            "Epoch 3/10 - Time: 5.94 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0699 - accuracy: 0.9788\n",
            "Epoch 4/10 - Time: 3.06 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0565 - accuracy: 0.9821\n",
            "Epoch 5/10 - Time: 3.27 seconds\n",
            "469/469 [==============================] - 3s 7ms/step - loss: 0.0460 - accuracy: 0.9860\n",
            "Epoch 6/10 - Time: 4.16 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0364 - accuracy: 0.9893\n",
            "Epoch 7/10 - Time: 3.22 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0303 - accuracy: 0.9905\n",
            "Epoch 8/10 - Time: 3.21 seconds\n",
            "469/469 [==============================] - 2s 5ms/step - loss: 0.0239 - accuracy: 0.9930\n",
            "Epoch 9/10 - Time: 3.31 seconds\n",
            "469/469 [==============================] - 3s 6ms/step - loss: 0.0219 - accuracy: 0.9936\n",
            "Epoch 10/10 - Time: 5.96 seconds\n",
            "Test accuracy: 0.9794999957084656\n",
            "Total training time: 41.68 seconds\n",
            "Training parts in parallel and evaluating combined model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 64 and 784 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_26/dense_31/Relu, cond/Identity_1)' with input shapes: [?,64], [?,784].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9b0150ed949b>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training parts in parallel and evaluating combined model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mparallel_model_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_parallel_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train parts in parallel and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Measure end time for the entire process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-9b0150ed949b>\u001b[0m in \u001b[0;36mtrain_and_evaluate_parallel_parts\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;31m# Train part1 to generate good features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mpart1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Generate intermediate outputs using part1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 1706, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 64 and 784 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](model_26/dense_31/Relu, cond/Identity_1)' with input shapes: [?,64], [?,784].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, datasets\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "import datetime\n",
        "\n",
        "def download_cifar10_data():\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def create_part1():\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    outputs = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_part2():\n",
        "    inputs = tf.keras.Input(shape=(16, 16, 64))\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    outputs = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_part3():\n",
        "    inputs = tf.keras.Input(shape=(8, 8, 256))\n",
        "    x = layers.Flatten()(inputs)\n",
        "    x = layers.Dense(512, activation='relu')(x)\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def create_combined_model(part1, part2, part3):\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    x = part1(inputs)\n",
        "    x = part2(x)\n",
        "    outputs = part3(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train_part1(x_train, y_train, epochs, batch_size, return_dict, event):\n",
        "    part1 = create_part1()\n",
        "    part1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    part1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "    intermediate_output = part1.predict(x_train)\n",
        "    return_dict['part1'] = part1\n",
        "    return_dict['intermediate_output1'] = intermediate_output\n",
        "    event.set()\n",
        "\n",
        "def train_part2(y_train, epochs, batch_size, return_dict, event1, event2):\n",
        "    event1.wait()\n",
        "    intermediate_output1 = return_dict['intermediate_output1']\n",
        "    part2 = create_part2()\n",
        "    part2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    part2.fit(intermediate_output1, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "    intermediate_output2 = part2.predict(intermediate_output1)\n",
        "    return_dict['part2'] = part2\n",
        "    return_dict['intermediate_output2'] = intermediate_output2\n",
        "    event2.set()\n",
        "\n",
        "def train_part3(y_train, epochs, batch_size, return_dict, event2):\n",
        "    event2.wait()\n",
        "    intermediate_output2 = return_dict['intermediate_output2']\n",
        "    part3 = create_part3()\n",
        "    part3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    part3.fit(intermediate_output2, y_train, batch_size=batch_size, epochs=epochs, verbose=1)\n",
        "    return_dict['part3'] = part3\n",
        "\n",
        "def train_and_evaluate_parallel_parts():\n",
        "    (x_train, y_train), (x_test, y_test) = download_cifar10_data()\n",
        "\n",
        "    epochs = 10\n",
        "    batch_size = 128\n",
        "\n",
        "    manager = multiprocessing.Manager()\n",
        "    return_dict = manager.dict()\n",
        "    event1 = multiprocessing.Event()\n",
        "    event2 = multiprocessing.Event()\n",
        "\n",
        "    process1 = multiprocessing.Process(target=train_part1, args=(x_train, y_train, epochs, batch_size, return_dict, event1))\n",
        "    process2 = multiprocessing.Process(target=train_part2, args=(y_train, epochs, batch_size, return_dict, event1, event2))\n",
        "    process3 = multiprocessing.Process(target=train_part3, args=(y_train, epochs, batch_size, return_dict, event2))\n",
        "\n",
        "    process1.start()\n",
        "    process2.start()\n",
        "    process3.start()\n",
        "\n",
        "    process1.join()\n",
        "    process2.join()\n",
        "    process3.join()\n",
        "\n",
        "    part1 = return_dict['part1']\n",
        "    part2 = return_dict['part2']\n",
        "    part3 = return_dict['part3']\n",
        "\n",
        "    combined_model = create_combined_model(part1, part2, part3)\n",
        "\n",
        "    intermediate_output_test = part1.predict(x_test)\n",
        "    intermediate_output_test = part2.predict(intermediate_output_test)\n",
        "    test_loss, test_acc = evaluate_model(combined_model, x_test, y_test)\n",
        "    print(f'Test accuracy (parallel training): {test_acc}')\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    print(\"Training parts in parallel and evaluating combined model...\")\n",
        "    parallel_model_acc = train_and_evaluate_parallel_parts()\n",
        "\n",
        "    end_time = datetime.datetime.now()\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "    print(f\"Total time for the entire process: {total_time:.2f} seconds\")\n",
        "    print(f\"Parallel training - Test accuracy: {parallel_model_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 825
        },
        "id": "-PjRDWV5WABQ",
        "outputId": "6665d2c4-3c7d-43ff-dbd2-db312b173f4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training parts in parallel and evaluating combined model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process Process-17:\n",
            "Process Process-18:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-6-eedabde3ee80>\", line 72, in train_part3\n",
            "    event2.wait()\n",
            "  File \"<ipython-input-6-eedabde3ee80>\", line 61, in train_part2\n",
            "    event1.wait()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 349, in wait\n",
            "    self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 349, in wait\n",
            "    self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
            "    return self._wait_semaphore.acquire(True, timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
            "    return self._wait_semaphore.acquire(True, timeout)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-eedabde3ee80>\u001b[0m in \u001b[0;36m<cell line: 115>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training parts in parallel and evaluating combined model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mparallel_model_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_parallel_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-eedabde3ee80>\u001b[0m in \u001b[0;36mtrain_and_evaluate_parallel_parts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mprocess1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mprocess2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mprocess3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  # Import TensorFlow library for building and training the neural network\n",
        "from tensorflow.keras import layers, models, datasets  # Import layers, models, and datasets from Keras\n",
        "import numpy as np  # Import NumPy for numerical operations\n",
        "import multiprocessing  # Import multiprocessing for parallel processing\n",
        "import datetime  # Import datetime for timing\n",
        "import logging  # Import logging for debug messages\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def download_cifar10_data():\n",
        "    logging.info(\"Downloading CIFAR-10 data...\")\n",
        "    # Load the CIFAR-10 dataset\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "    # Normalize the pixel values to be between 0 and 1\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_test = x_test.astype('float32') / 255.0\n",
        "    # Convert class labels to one-hot encoded format\n",
        "    y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "    y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "    logging.info(\"CIFAR-10 data downloaded and preprocessed.\")\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "def create_part1():\n",
        "    logging.info(\"Creating Part 1 of the model...\")\n",
        "    # Define the first part of the model\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))  # Input layer for CIFAR-10 images\n",
        "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)  # Convolutional layer\n",
        "    x = layers.MaxPooling2D((2, 2))(x)  # Max pooling layer\n",
        "    outputs = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the model\n",
        "    logging.info(\"Part 1 created.\")\n",
        "    return model\n",
        "\n",
        "def create_part2():\n",
        "    logging.info(\"Creating Part 2 of the model...\")\n",
        "    # Define the second part of the model\n",
        "    inputs = tf.keras.Input(shape=(16, 16, 64))  # Input layer for intermediate output\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(inputs)  # Convolutional layer\n",
        "    x = layers.MaxPooling2D((2, 2))(x)  # Max pooling layer\n",
        "    outputs = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)  # Convolutional layer\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the model\n",
        "    logging.info(\"Part 2 created.\")\n",
        "    return model\n",
        "\n",
        "def create_part3():\n",
        "    logging.info(\"Creating Part 3 of the model...\")\n",
        "    # Define the third part of the model\n",
        "    inputs = tf.keras.Input(shape=(8, 8, 256))  # Input layer for intermediate output\n",
        "    x = layers.Flatten()(inputs)  # Flatten layer to convert 3D output to 1D\n",
        "    x = layers.Dense(512, activation='relu')(x)  # Fully connected layer\n",
        "    outputs = layers.Dense(10, activation='softmax')(x)  # Output layer for classification\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the model\n",
        "    logging.info(\"Part 3 created.\")\n",
        "    return model\n",
        "\n",
        "def create_combined_model(part1, part2, part3):\n",
        "    logging.info(\"Combining parts into a single model...\")\n",
        "    # Combine part1, part2, and part3 into a single model\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))  # Input layer for CIFAR-10 images\n",
        "    x = part1(inputs)  # Pass input through part1\n",
        "    x = part2(x)  # Pass output of part1 through part2\n",
        "    outputs = part3(x)  # Pass output of part2 through part3\n",
        "    model = tf.keras.Model(inputs, outputs)  # Create the combined model\n",
        "    logging.info(\"Combined model created.\")\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, x_test, y_test):\n",
        "    logging.info(\"Evaluating the model...\")\n",
        "    # Evaluate the model on the test dataset\n",
        "    test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)  # Evaluate and get the loss and accuracy\n",
        "    logging.info(f\"Model evaluation completed. Test accuracy: {test_acc}\")\n",
        "    return test_loss, test_acc\n",
        "\n",
        "def train_part1(x_train, y_train, epochs, batch_size, return_dict, event):\n",
        "    logging.info(\"Training Part 1...\")\n",
        "    # Train part1 of the model\n",
        "    part1 = create_part1()  # Create part1 model\n",
        "    part1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "    part1.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)  # Train the model\n",
        "    intermediate_output = part1.predict(x_train)  # Get the intermediate output\n",
        "    return_dict['part1'] = part1  # Store the trained part1 model in the shared dictionary\n",
        "    return_dict['intermediate_output1'] = intermediate_output  # Store the intermediate output in the shared dictionary\n",
        "    logging.info(\"Part 1 training completed.\")\n",
        "    event.set()  # Signal that part1 has finished training\n",
        "\n",
        "def train_part2(y_train, epochs, batch_size, return_dict, event1, event2):\n",
        "    logging.info(\"Waiting for Part 1 to finish...\")\n",
        "    # Wait for part1 to finish and the intermediate output to be ready\n",
        "    event1.wait()\n",
        "    logging.info(\"Training Part 2...\")\n",
        "    intermediate_output1 = return_dict['intermediate_output1']  # Retrieve intermediate output from part1\n",
        "    part2 = create_part2()  # Create part2 model\n",
        "    part2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "    part2.fit(intermediate_output1, y_train, batch_size=batch_size, epochs=epochs, verbose=1)  # Train the model\n",
        "    intermediate_output2 = part2.predict(intermediate_output1)  # Get the intermediate output\n",
        "    return_dict['part2'] = part2  # Store the trained part2 model in the shared dictionary\n",
        "    return_dict['intermediate_output2'] = intermediate_output2  # Store the intermediate output in the shared dictionary\n",
        "    logging.info(\"Part 2 training completed.\")\n",
        "    event2.set()  # Signal that part2 has finished training\n",
        "\n",
        "def train_part3(y_train, epochs, batch_size, return_dict, event2):\n",
        "    logging.info(\"Waiting for Part 2 to finish...\")\n",
        "    # Wait for part2 to finish and the intermediate output to be ready\n",
        "    event2.wait()\n",
        "    logging.info(\"Training Part 3...\")\n",
        "    intermediate_output2 = return_dict['intermediate_output2']  # Retrieve intermediate output from part2\n",
        "    part3 = create_part3()  # Create part3 model\n",
        "    part3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # Compile the model\n",
        "    part3.fit(intermediate_output2, y_train, batch_size=batch_size, epochs=epochs, verbose=1)  # Train the model\n",
        "    return_dict['part3'] = part3  # Store the trained part3 model in the shared dictionary\n",
        "    logging.info(\"Part 3 training completed.\")\n",
        "\n",
        "def train_and_evaluate_parallel_parts():\n",
        "    logging.info(\"Starting training and evaluation of parallel parts...\")\n",
        "    # Load and preprocess the CIFAR-10 dataset\n",
        "    (x_train, y_train), (x_test, y_test) = download_cifar10_data()\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 10\n",
        "    batch_size = 128\n",
        "\n",
        "    # Create a manager for shared dictionary and events for synchronization\n",
        "    manager = multiprocessing.Manager()\n",
        "    return_dict = manager.dict()  # Shared dictionary to store results from processes\n",
        "    event1 = multiprocessing.Event()  # Event to signal when part1 has finished training\n",
        "    event2 = multiprocessing.Event()  # Event to signal when part2 has finished training\n",
        "\n",
        "    # Create processes for training part1, part2, and part3\n",
        "    process1 = multiprocessing.Process(target=train_part1, args=(x_train, y_train, epochs, batch_size, return_dict, event1))\n",
        "    process2 = multiprocessing.Process(target=train_part2, args=(y_train, epochs, batch_size, return_dict, event1, event2))\n",
        "    process3 = multiprocessing.Process(target=train_part3, args=(y_train, epochs, batch_size, return_dict, event2))\n",
        "\n",
        "    # Start the training processes\n",
        "    process1.start()  # Start training part1\n",
        "    process2.start()  # Start training part2\n",
        "    process3.start()  # Start training part3\n",
        "\n",
        "    # Wait for all processes to complete\n",
        "    process1.join()\n",
        "    process2.join()\n",
        "    process3.join()\n",
        "\n",
        "    # Retrieve the trained parts from the shared dictionary\n",
        "    part1 = return_dict['part1']\n",
        "    part2 = return_dict['part2']\n",
        "    part3 = return_dict['part3']\n",
        "\n",
        "    # Combine part1, part2, and part3 for evaluation\n",
        "    combined_model = create_combined_model(part1, part2, part3)\n",
        "\n",
        "    # Evaluate the combined model on the test dataset\n",
        "    intermediate_output_test = part1.predict(x_test)  # Get intermediate output for test data from part1\n",
        "    intermediate_output_test = part2.predict(intermediate_output_test)  # Get intermediate output for test data from part2\n",
        "    test_loss, test_acc = evaluate_model(combined_model, x_test, y_test)  # Evaluate the combined model with test data\n",
        "    logging.info(f'Test accuracy (parallel training): {test_acc}')\n",
        "\n",
        "    return test_acc\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Measure time for the entire process\n",
        "    start_time = datetime.datetime.now()\n",
        "\n",
        "    logging.info(\"Training parts in parallel and evaluating combined model...\")\n",
        "    parallel_model_acc = train_and_evaluate_parallel_parts()  # Train parts in parallel and evaluate the model\n",
        "\n",
        "    # Measure end time for the entire process\n",
        "    end_time = datetime.datetime.now()\n",
        "    # Calculate total time for the entire process\n",
        "    total_time = (end_time - start_time).total_seconds()\n",
        "    logging.info(f\"Total time for the entire process: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Print the final accuracy\n",
        "    logging.info(f\"Parallel training - Test accuracy: {parallel_model_acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "CJVKDsxOXlal",
        "outputId": "9170fa5a-30b4-4887-ca58-3c3f7375f680"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Process Process-13:\n",
            "Process Process-14:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-5-a7a042cc427d>\", line 105, in train_part3\n",
            "    event2.wait()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 349, in wait\n",
            "    self._cond.wait(timeout)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
            "    return self._wait_semaphore.acquire(True, timeout)\n",
            "  File \"<ipython-input-5-a7a042cc427d>\", line 90, in train_part2\n",
            "    event1.wait()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 349, in wait\n",
            "    self._cond.wait(timeout)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.10/multiprocessing/synchronize.py\", line 261, in wait\n",
            "    return self._wait_semaphore.acquire(True, timeout)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a7a042cc427d>\u001b[0m in \u001b[0;36m<cell line: 160>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training parts in parallel and evaluating combined model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mparallel_model_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_parallel_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train parts in parallel and evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;31m# Measure end time for the entire process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-a7a042cc427d>\u001b[0m in \u001b[0;36mtrain_and_evaluate_parallel_parts\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;31m# Wait for all processes to complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprocess1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mprocess2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0mprocess3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}