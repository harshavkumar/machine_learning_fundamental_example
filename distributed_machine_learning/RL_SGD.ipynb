{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "SGD"
      ],
      "metadata": {
        "id": "fuXwua5-Ce52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the states in a simple grid world\n",
        "states = np.array([0, 1, 2, 3, 4])  # 5 states\n",
        "\n",
        "# Define feature vectors for each state (for simplicity, we'll use the state index as the feature)\n",
        "def feature_vector(state):\n",
        "    return np.array([state])\n",
        "\n",
        "# Initialize weights randomly\n",
        "initial_weights = np.random.randn(1)\n",
        "\n",
        "# Set the true value of each state (for demonstration purposes)\n",
        "true_values = np.array([0, 1, 2, 3, 4])  # True value is simply the state index\n",
        "\n",
        "# Define the learning rate\n",
        "alpha = 0.01\n",
        "\n",
        "# Function to compute value estimate\n",
        "def value_estimate(state, weights):\n",
        "    return np.dot(weights, feature_vector(state))\n",
        "\n",
        "\n",
        "### Batch Gradient Descent ###\n",
        "print(\"\\nBatch Gradient Descent\\n\" + \"-\"*25)\n",
        "\n",
        "weights = initial_weights.copy()\n",
        "print(f\"Initial Weights: {weights}\")\n",
        "\n",
        "# Perform Gradient Descent update for multiple iterations\n",
        "n_iterations = 10  # Number of iterations\n",
        "for iteration in range(n_iterations):\n",
        "    gradient_sum = np.zeros_like(weights)\n",
        "    for state in states:\n",
        "        estimate = value_estimate(state, weights)\n",
        "        true_value = true_values[state]\n",
        "        error = true_value - estimate\n",
        "        gradient = feature_vector(state)\n",
        "        gradient_sum += error * gradient\n",
        "\n",
        "    # Update weights\n",
        "    weights += alpha * gradient_sum\n",
        "\n",
        "    print(f\"Weights after iteration {iteration + 1}: {weights}\")\n",
        "\n",
        "\n",
        "### Stochastic Gradient Descent (SGD) ###\n",
        "print(\"\\nStochastic Gradient Descent (SGD)\\n\" + \"-\"*40)\n",
        "\n",
        "weights = initial_weights.copy()\n",
        "print(f\"Initial Weights: {weights}\")\n",
        "\n",
        "# Perform Gradient Descent updates for each state individually (SGD) for multiple iterations\n",
        "n_iterations = 10  # Number of iterations\n",
        "for iteration in range(n_iterations):\n",
        "    for state in states:\n",
        "        estimate = value_estimate(state, weights)\n",
        "        true_value = true_values[state]\n",
        "        error = true_value - estimate\n",
        "        gradient = feature_vector(state)\n",
        "\n",
        "        # Update weights\n",
        "        weights += alpha * error * gradient\n",
        "\n",
        "    print(f\"Weights after iteration {iteration + 1}: {weights}\")\n",
        "\n",
        "\n",
        "### Mini-Batch Gradient Descent ###\n",
        "print(\"\\nMini-Batch Gradient Descent\\n\" + \"-\"*35)\n",
        "\n",
        "weights = initial_weights.copy()\n",
        "print(f\"Initial Weights: {weights}\")\n",
        "\n",
        "batch_size = 2  # Define the size of the mini-batch\n",
        "\n",
        "# Perform Gradient Descent updates for mini-batches for multiple iterations\n",
        "n_iterations = 10  # Number of iterations\n",
        "for iteration in range(n_iterations):\n",
        "    for i in range(0, len(states), batch_size):\n",
        "        gradient_sum = np.zeros_like(weights)\n",
        "        mini_batch_states = states[i:i + batch_size]\n",
        "\n",
        "        for state in mini_batch_states:\n",
        "            estimate = value_estimate(state, weights)\n",
        "            true_value = true_values[state]\n",
        "            error = true_value - estimate\n",
        "            gradient = feature_vector(state)\n",
        "            gradient_sum += error * gradient\n",
        "\n",
        "        # Update weights\n",
        "        weights += alpha * gradient_sum\n",
        "\n",
        "    print(f\"Weights after iteration {iteration + 1}: {weights}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3DQ0fsSCdpL",
        "outputId": "f45f1e24-5380-4627-8d90-68273714be20"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch Gradient Descent\n",
            "-------------------------\n",
            "Initial Weights: [1.75781712]\n",
            "Weights after iteration 1: [1.53047199]\n",
            "Weights after iteration 2: [1.37133039]\n",
            "Weights after iteration 3: [1.25993127]\n",
            "Weights after iteration 4: [1.18195189]\n",
            "Weights after iteration 5: [1.12736632]\n",
            "Weights after iteration 6: [1.08915643]\n",
            "Weights after iteration 7: [1.0624095]\n",
            "Weights after iteration 8: [1.04368665]\n",
            "Weights after iteration 9: [1.03058065]\n",
            "Weights after iteration 10: [1.02140646]\n",
            "\n",
            "Stochastic Gradient Descent (SGD)\n",
            "----------------------------------------\n",
            "Initial Weights: [1.75781712]\n",
            "Weights after iteration 1: [1.55054335]\n",
            "Weights after iteration 2: [1.3999619]\n",
            "Weights after iteration 3: [1.29056663]\n",
            "Weights after iteration 4: [1.21109252]\n",
            "Weights after iteration 5: [1.15335571]\n",
            "Weights after iteration 6: [1.11141074]\n",
            "Weights after iteration 7: [1.08093831]\n",
            "Weights after iteration 8: [1.05880053]\n",
            "Weights after iteration 9: [1.04271775]\n",
            "Weights after iteration 10: [1.03103384]\n",
            "\n",
            "Mini-Batch Gradient Descent\n",
            "-----------------------------------\n",
            "Initial Weights: [1.75781712]\n",
            "Weights after iteration 1: [1.54827463]\n",
            "Weights after iteration 2: [1.39667231]\n",
            "Weights after iteration 3: [1.28698924]\n",
            "Weights after iteration 4: [1.20763442]\n",
            "Weights after iteration 5: [1.15022184]\n",
            "Weights after iteration 6: [1.1086843]\n",
            "Weights after iteration 7: [1.07863222]\n",
            "Weights after iteration 8: [1.05688978]\n",
            "Weights after iteration 9: [1.0411593]\n",
            "Weights after iteration 10: [1.02977843]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSyGtW2MaklN",
        "outputId": "29a3b8e9-d0ab-4ddf-b619-01211d7454a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Weights: [0.1588437]\n",
            "Episode 2, Weights: [0.16077595]\n",
            "Episode 3, Weights: [0.16079946]\n",
            "Episode 4, Weights: [0.16079974]\n",
            "Episode 5, Weights: [0.16079975]\n",
            "Episode 6, Weights: [0.16079975]\n",
            "Episode 7, Weights: [0.16079975]\n",
            "Episode 8, Weights: [0.16079975]\n",
            "Episode 9, Weights: [0.16079975]\n",
            "Episode 10, Weights: [0.16079975]\n",
            "Episode 11, Weights: [0.16079975]\n",
            "Episode 12, Weights: [0.16079975]\n",
            "Episode 13, Weights: [0.16079975]\n",
            "Episode 14, Weights: [0.16079975]\n",
            "Episode 15, Weights: [0.16079975]\n",
            "Episode 16, Weights: [0.16079975]\n",
            "Episode 17, Weights: [0.16079975]\n",
            "Episode 18, Weights: [0.16079975]\n",
            "Episode 19, Weights: [0.16079975]\n",
            "Episode 20, Weights: [0.16079975]\n",
            "Episode 21, Weights: [0.16079975]\n",
            "Episode 22, Weights: [0.16079975]\n",
            "Episode 23, Weights: [0.16079975]\n",
            "Episode 24, Weights: [0.16079975]\n",
            "Episode 25, Weights: [0.16079975]\n",
            "Episode 26, Weights: [0.16079975]\n",
            "Episode 27, Weights: [0.16079975]\n",
            "Episode 28, Weights: [0.16079975]\n",
            "Episode 29, Weights: [0.16079975]\n",
            "Episode 30, Weights: [0.16079975]\n",
            "Episode 31, Weights: [0.16079975]\n",
            "Episode 32, Weights: [0.16079975]\n",
            "Episode 33, Weights: [0.16079975]\n",
            "Episode 34, Weights: [0.16079975]\n",
            "Episode 35, Weights: [0.16079975]\n",
            "Episode 36, Weights: [0.16079975]\n",
            "Episode 37, Weights: [0.16079975]\n",
            "Episode 38, Weights: [0.16079975]\n",
            "Episode 39, Weights: [0.16079975]\n",
            "Episode 40, Weights: [0.16079975]\n",
            "Episode 41, Weights: [0.16079975]\n",
            "Episode 42, Weights: [0.16079975]\n",
            "Episode 43, Weights: [0.16079975]\n",
            "Episode 44, Weights: [0.16079975]\n",
            "Episode 45, Weights: [0.16079975]\n",
            "Episode 46, Weights: [0.16079975]\n",
            "Episode 47, Weights: [0.16079975]\n",
            "Episode 48, Weights: [0.16079975]\n",
            "Episode 49, Weights: [0.16079975]\n",
            "Episode 50, Weights: [0.16079975]\n",
            "Episode 51, Weights: [0.16079975]\n",
            "Episode 52, Weights: [0.16079975]\n",
            "Episode 53, Weights: [0.16079975]\n",
            "Episode 54, Weights: [0.16079975]\n",
            "Episode 55, Weights: [0.16079975]\n",
            "Episode 56, Weights: [0.16079975]\n",
            "Episode 57, Weights: [0.16079975]\n",
            "Episode 58, Weights: [0.16079975]\n",
            "Episode 59, Weights: [0.16079975]\n",
            "Episode 60, Weights: [0.16079975]\n",
            "Episode 61, Weights: [0.16079975]\n",
            "Episode 62, Weights: [0.16079975]\n",
            "Episode 63, Weights: [0.16079975]\n",
            "Episode 64, Weights: [0.16079975]\n",
            "Episode 65, Weights: [0.16079975]\n",
            "Episode 66, Weights: [0.16079975]\n",
            "Episode 67, Weights: [0.16079975]\n",
            "Episode 68, Weights: [0.16079975]\n",
            "Episode 69, Weights: [0.16079975]\n",
            "Episode 70, Weights: [0.16079975]\n",
            "Episode 71, Weights: [0.16079975]\n",
            "Episode 72, Weights: [0.16079975]\n",
            "Episode 73, Weights: [0.16079975]\n",
            "Episode 74, Weights: [0.16079975]\n",
            "Episode 75, Weights: [0.16079975]\n",
            "Episode 76, Weights: [0.16079975]\n",
            "Episode 77, Weights: [0.16079975]\n",
            "Episode 78, Weights: [0.16079975]\n",
            "Episode 79, Weights: [0.16079975]\n",
            "Episode 80, Weights: [0.16079975]\n",
            "Episode 81, Weights: [0.16079975]\n",
            "Episode 82, Weights: [0.16079975]\n",
            "Episode 83, Weights: [0.16079975]\n",
            "Episode 84, Weights: [0.16079975]\n",
            "Episode 85, Weights: [0.16079975]\n",
            "Episode 86, Weights: [0.16079975]\n",
            "Episode 87, Weights: [0.16079975]\n",
            "Episode 88, Weights: [0.16079975]\n",
            "Episode 89, Weights: [0.16079975]\n",
            "Episode 90, Weights: [0.16079975]\n",
            "Episode 91, Weights: [0.16079975]\n",
            "Episode 92, Weights: [0.16079975]\n",
            "Episode 93, Weights: [0.16079975]\n",
            "Episode 94, Weights: [0.16079975]\n",
            "Episode 95, Weights: [0.16079975]\n",
            "Episode 96, Weights: [0.16079975]\n",
            "Episode 97, Weights: [0.16079975]\n",
            "Episode 98, Weights: [0.16079975]\n",
            "Episode 99, Weights: [0.16079975]\n",
            "Episode 100, Weights: [0.16079975]\n",
            "Final weights: [0.16079975]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assume we have an environment simulator and a policy π\n",
        "# For simplicity, let's define a simple environment and policy\n",
        "def simple_policy(state):\n",
        "    # A dummy policy that always takes the same action\n",
        "    return 0  # Always take action 0\n",
        "\n",
        "def environment_step(state, action):\n",
        "    # A dummy environment transition, returns next_state, reward, done\n",
        "    next_state = state + 1\n",
        "    reward = 1.0  # Constant reward\n",
        "    done = next_state == 10  # End the episode at state 10\n",
        "    return next_state, reward, done\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.01  # Learning rate\n",
        "num_episodes = 100  # Number of episodes to run\n",
        "state_dim = 1  # Dimension of state feature vector\n",
        "\n",
        "# Initialize value function weights (w) as appropriate\n",
        "w = np.zeros(state_dim)  # Start with w = 0\n",
        "\n",
        "# Feature function for state (simple linear feature in this case)\n",
        "def feature_function(state):\n",
        "    return np.array([state])\n",
        "\n",
        "# Main loop: Repeat for each episode\n",
        "for episode in range(num_episodes):\n",
        "    # Generate an episode S_0, A_0, R_1, S_1, A_1, ..., R_T, S_T using π\n",
        "    state = 0  # Start at state 0\n",
        "    episode_data = []\n",
        "    while True:\n",
        "        action = simple_policy(state)\n",
        "        next_state, reward, done = environment_step(state, action)\n",
        "        episode_data.append((state, reward))\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    # Calculate returns G_t for the episode\n",
        "    returns = []\n",
        "    G = 0\n",
        "    for state, reward in reversed(episode_data):\n",
        "        G = reward + G  # Monte Carlo return is cumulative reward\n",
        "        returns.insert(0, G)  # Prepend the return\n",
        "\n",
        "    # For t = 0, 1, ..., T - 1:\n",
        "    for t, (state, G_t) in enumerate(zip([s for s, _ in episode_data], returns)):\n",
        "        # Compute the estimated value v_hat(S_t, w)\n",
        "        x_t = feature_function(state)\n",
        "        v_hat = np.dot(w, x_t)\n",
        "\n",
        "        # Compute the gradient of v_hat with respect to w\n",
        "        gradient = x_t  # For linear approximation, ∇v_hat is simply the feature vector\n",
        "\n",
        "        # Update the weights w using the gradient descent update rule\n",
        "        w += alpha * (G_t - v_hat) * gradient\n",
        "\n",
        "    # Optionally, print weights after each episode\n",
        "    print(f\"Episode {episode + 1}, Weights: {w}\")\n",
        "\n",
        "# Final value function weights\n",
        "print(\"Final weights:\", w)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define states\n",
        "states = ['S1', 'S2', 'S3']\n",
        "\n",
        "# Define rewards for state transitions\n",
        "rewards = {'S1': 1, 'S2': 2}\n",
        "\n",
        "# Define transition probabilities (for simplicity, deterministic transitions)\n",
        "transitions = {\n",
        "    'S1': 'S2',\n",
        "    'S2': 'S3'\n",
        "}\n",
        "\n",
        "# Initialize value function\n",
        "V = {state: 0 for state in states}\n",
        "\n",
        "# Parameters\n",
        "gamma = 0.9  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "\n",
        "# Function to simulate n-step TD\n",
        "def n_step_td(n, episodes=10):\n",
        "    for episode in range(episodes):\n",
        "        state = 'S1'\n",
        "        t = 0\n",
        "        states_visited = [state]\n",
        "        rewards_received = []\n",
        "\n",
        "        while state != 'S3':\n",
        "            next_state = transitions[state]\n",
        "            reward = rewards[state]\n",
        "            rewards_received.append(reward)\n",
        "            states_visited.append(next_state)\n",
        "            state = next_state\n",
        "            t += 1\n",
        "\n",
        "            if len(rewards_received) >= n:\n",
        "                G = sum([gamma**i * rewards_received[i] for i in range(n)])\n",
        "                if state != 'S3':\n",
        "                    G += gamma**n * V[state]\n",
        "\n",
        "                update_state = states_visited[t-n]\n",
        "                V[update_state] += alpha * (G - V[update_state])\n",
        "\n",
        "                rewards_received = rewards_received[1:]\n",
        "                states_visited = states_visited[1:]\n",
        "\n",
        "        # Final update if episode ends before n steps\n",
        "        for tau in range(len(rewards_received)):\n",
        "            G = sum([gamma**i * rewards_received[i] for i in range(tau, len(rewards_received))])\n",
        "            update_state = states_visited[tau]\n",
        "            V[update_state] += alpha * (G - V[update_state])\n",
        "\n",
        "# Run n-step TD for n = 1, 2, 3\n",
        "print(\"Initial Value Function:\", V)\n",
        "for n in [1, 2, 3]:\n",
        "    print(f\"\\nRunning {n}-step TD...\")\n",
        "    n_step_td(n, episodes=10)\n",
        "    print(f\"Value Function after {n}-step TD:\", V)\n",
        "\n",
        "# Final value function\n",
        "print(\"\\nFinal Value Function:\", V)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ekz6rOVkGbr",
        "outputId": "0ecfca9d-31dc-42e1-e0b1-7cea0271acc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Value Function: {'S1': 0, 'S2': 0, 'S3': 0}\n",
            "\n",
            "Running 1-step TD...\n",
            "Value Function after 1-step TD: {'S1': 0.6513215599000001, 'S2': 0, 'S3': 1.3026431198000001}\n",
            "\n",
            "Running 2-step TD...\n",
            "Value Function after 2-step TD: {'S1': 2.0508021532294305, 'S2': 1.3026431198000001, 'S3': 1.3026431198000001}\n",
            "\n",
            "Running 3-step TD...\n",
            "Value Function after 3-step TD: {'S1': 2.5387708634617594, 'S2': 1.6265823788388616, 'S3': 1.3026431198000001}\n",
            "\n",
            "Final Value Function: {'S1': 2.5387708634617594, 'S2': 1.6265823788388616, 'S3': 1.3026431198000001}\n"
          ]
        }
      ]
    }
  ]
}