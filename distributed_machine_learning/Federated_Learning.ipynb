{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-_oX42jyjrLB",
        "outputId": "643964b6-5762-455d-f856-988080954b82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.86.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (5.5.0)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (0.1.8)\n",
            "Collecting dp-accounting==0.4.3 (from tensorflow-federated)\n",
            "  Downloading dp_accounting-0.4.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting google-vizier==0.1.11 (from tensorflow-federated)\n",
            "  Downloading google_vizier-0.1.11-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated)\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "INFO: pip is looking at multiple versions of tensorflow-federated to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-federated\n",
            "  Downloading tensorflow_federated-0.85.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "  Downloading tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Collecting portpicker~=1.6 (from tensorflow-federated)\n",
            "  Downloading portpicker-1.6.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting scipy~=1.9.3 (from tensorflow-federated)\n",
            "  Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-model-optimization==0.7.5 (from tensorflow-federated)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl.metadata (914 bytes)\n",
            "Collecting tensorflow-privacy==0.9.0 (from tensorflow-federated)\n",
            "  Downloading tensorflow_privacy-0.9.0-py3-none-any.whl.metadata (763 bytes)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.10/dist-packages (from tensorflow-federated) (4.66.5)\n",
            "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting googleapis-common-protos==1.61.0 (from tensorflow-federated)\n",
            "  Downloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting ml-dtypes==0.2.0 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow)\n",
            "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n",
            "  Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.15,>=2.14.0 (from tensorflow)\n",
            "  Downloading keras-2.14.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.10/dist-packages (from dp-accounting==0.4.3->tensorflow-federated) (1.3.0)\n",
            "Collecting attrs~=23.1 (from tensorflow-federated)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.66.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting sqlalchemy<=1.4.20,>=1.4 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading SQLAlchemy-1.4.20.tar.gz (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging (from tensorflow)\n",
            "  Downloading packaging-22.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scikit-learn==1.*,>=1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-privacy==0.9.0->tensorflow-federated) (1.3.2)\n",
            "Collecting tensorflow-probability~=0.22.0 (from tensorflow-privacy==0.9.0->tensorflow-federated)\n",
            "  Downloading tensorflow_probability-0.22.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0->tensorflow-federated) (3.5.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker~=1.6->tensorflow-federated) (5.9.5)\n",
            "Collecting numpy~=1.25 (from tensorflow-federated)\n",
            "  Downloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.27.0)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "INFO: pip is looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-tools>=1.35.0 (from google-vizier==0.1.11->tensorflow-federated)\n",
            "  Downloading grpcio_tools-1.66.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.65.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-tools to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_tools-1.64.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.63.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "  Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow)\n",
            "  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated) (3.0.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0->tensorflow-federated) (2.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Downloading tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl (71.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.8/71.8 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.9/489.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dp_accounting-0.4.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_vizier-0.1.11-py3-none-any.whl (721 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.6/721.6 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading googleapis_common_protos-1.61.0-py2.py3-none-any.whl (230 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.9/230.9 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.14-cp310-cp310-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_privacy-0.9.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.2/323.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-22.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portpicker-1.6.0-py3-none-any.whl (16 kB)\n",
            "Downloading scipy-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.7/33.7 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading grpcio_tools-1.62.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_probability-0.22.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax, sqlalchemy\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535360 sha256=df56f2551ec03ad743d5ce7254be216ef6e654f854128b22da91e2ba6052f74b\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/52/e7/dfa571c9f9b879e3facaa1584f52be04c4c3d1e14054ef40ab\n",
            "  Building wheel for sqlalchemy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlalchemy: filename=SQLAlchemy-1.4.20-cp310-cp310-linux_x86_64.whl size=1529852 sha256=b5e6256272b1ea76095db38cbcdbda48ebdacbb04e9315f7e11934efbb320ff5\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/42/20/a958989c470cc1a6fe1d1279b0193f0e508161327fc3d951d9\n",
            "Successfully built jax sqlalchemy\n",
            "Installing collected packages: wrapt, typing-extensions, tensorflow-estimator, sqlalchemy, protobuf, portpicker, packaging, numpy, keras, attrs, tensorflow-probability, tensorflow-model-optimization, scipy, ml-dtypes, grpcio-tools, googleapis-common-protos, jaxlib, jax, google-vizier, google-auth-oauthlib, dp-accounting, tensorboard, tensorflow, tensorflow-privacy, tensorflow-federated\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.16.0\n",
            "    Uninstalling wrapt-1.16.0:\n",
            "      Successfully uninstalled wrapt-1.16.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.32\n",
            "    Uninstalling SQLAlchemy-2.0.32:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.32\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: portpicker\n",
            "    Found existing installation: portpicker 1.5.2\n",
            "    Uninstalling portpicker-1.5.2:\n",
            "      Successfully uninstalled portpicker-1.5.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.4.1\n",
            "    Uninstalling keras-3.4.1:\n",
            "      Successfully uninstalled keras-3.4.1\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 24.2.0\n",
            "    Uninstalling attrs-24.2.0:\n",
            "      Successfully uninstalled attrs-24.2.0\n",
            "  Attempting uninstall: tensorflow-probability\n",
            "    Found existing installation: tensorflow-probability 0.24.0\n",
            "    Uninstalling tensorflow-probability-0.24.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.24.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.0\n",
            "    Uninstalling ml-dtypes-0.4.0:\n",
            "      Successfully uninstalled ml-dtypes-0.4.0\n",
            "  Attempting uninstall: googleapis-common-protos\n",
            "    Found existing installation: googleapis-common-protos 1.64.0\n",
            "    Uninstalling googleapis-common-protos-1.64.0:\n",
            "      Successfully uninstalled googleapis-common-protos-1.64.0\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.26+cuda12.cudnn89\n",
            "    Uninstalling jaxlib-0.4.26+cuda12.cudnn89:\n",
            "      Successfully uninstalled jaxlib-0.4.26+cuda12.cudnn89\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.26\n",
            "    Uninstalling jax-0.4.26:\n",
            "      Successfully uninstalled jax-0.4.26\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 1.2.1\n",
            "    Uninstalling google-auth-oauthlib-1.2.1:\n",
            "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.17.0\n",
            "    Uninstalling tensorboard-2.17.0:\n",
            "      Successfully uninstalled tensorboard-2.17.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.17.0\n",
            "    Uninstalling tensorflow-2.17.0:\n",
            "      Successfully uninstalled tensorflow-2.17.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.13 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "albumentations 1.4.14 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "albumentations 1.4.14 requires typing-extensions>=4.9.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "chex 0.1.86 requires jax>=0.4.16, but you have jax 0.4.14 which is incompatible.\n",
            "flax 0.8.4 requires jax>=0.4.19, but you have jax 0.4.14 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.20 which is incompatible.\n",
            "orbax-checkpoint 0.6.1 requires jax>=0.4.26, but you have jax 0.4.14 which is incompatible.\n",
            "pandas-stubs 2.1.4.231227 requires numpy>=1.26.0; python_version < \"3.13\", but you have numpy 1.25.2 which is incompatible.\n",
            "pydantic 2.8.2 requires typing-extensions>=4.6.1; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.20.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\n",
            "tensorstore 0.1.64 requires ml-dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.14.1 which is incompatible.\n",
            "torch 2.4.0+cu121 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "typeguard 4.3.0 requires typing-extensions>=4.10.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "xarray 2024.6.0 requires packaging>=23.1, but you have packaging 22.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 dp-accounting-0.4.3 google-auth-oauthlib-1.0.0 google-vizier-0.1.11 googleapis-common-protos-1.61.0 grpcio-tools-1.62.3 jax-0.4.14 jaxlib-0.4.14 keras-2.14.0 ml-dtypes-0.2.0 numpy-1.25.2 packaging-22.0 portpicker-1.6.0 protobuf-4.25.4 scipy-1.9.3 sqlalchemy-1.4.20 tensorboard-2.14.1 tensorflow-2.14.1 tensorflow-estimator-2.14.0 tensorflow-federated-0.84.0 tensorflow-model-optimization-0.7.5 tensorflow-privacy-0.9.0 tensorflow-probability-0.22.1 typing-extensions-4.5.0 wrapt-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "portpicker"
                ]
              },
              "id": "c9630d1edf3746abb12e08333c0ea579"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "pip install tensorflow tensorflow-federated"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Generate synthetic data for two hospitals (Hospital A and Hospital B)\n",
        "def generate_synthetic_data(num_samples, seed=0):\n",
        "    np.random.seed(seed)\n",
        "    x = np.random.rand(num_samples, 10)  # 10 features\n",
        "    y = np.random.randint(0, 2, num_samples)  # Binary classification target\n",
        "    return pd.DataFrame(x, columns=[f'feature_{i}' for i in range(10)]), pd.Series(y, name='target')\n",
        "\n",
        "# Generate data for two hospitals\n",
        "hospital_A_data, hospital_A_labels = generate_synthetic_data(5, seed=1)\n",
        "hospital_B_data, hospital_B_labels = generate_synthetic_data(5, seed=2)\n",
        "\n",
        "# Combine data as if it were centralized (Normal Data)\n",
        "centralized_data = pd.concat([hospital_A_data, hospital_B_data])\n",
        "centralized_labels = pd.concat([hospital_A_labels, hospital_B_labels])\n",
        "\n",
        "# Display centralized data\n",
        "print(\"Centralized Data (Normal Data):\")\n",
        "print(pd.concat([centralized_data, centralized_labels], axis=1))\n",
        "\n",
        "# Create TensorFlow Datasets for Federated Learning\n",
        "hospital_A_dataset = tf.data.Dataset.from_tensor_slices((hospital_A_data.values, hospital_A_labels.values)).batch(2)\n",
        "hospital_B_dataset = tf.data.Dataset.from_tensor_slices((hospital_B_data.values, hospital_B_labels.values)).batch(2)\n",
        "\n",
        "# Federated data is represented as a list of local datasets\n",
        "federated_data = [hospital_A_dataset, hospital_B_dataset]\n",
        "\n",
        "# Display federated data samples\n",
        "print(\"\\nFederated Data Sample from Hospital A:\")\n",
        "for x, y in hospital_A_dataset.take(1):\n",
        "    print(f\"Features: {x.numpy()}, Labels: {y.numpy()}\")\n",
        "\n",
        "print(\"\\nFederated Data Sample from Hospital B:\")\n",
        "for x, y in hospital_B_dataset.take(1):\n",
        "    print(f\"Features: {x.numpy()}, Labels: {y.numpy()}\")\n"
      ],
      "metadata": {
        "id": "PPAnNO1MlPwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "fKykyP33-65P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic data for two hospitals (Hospital A and Hospital B)\n",
        "def generate_synthetic_data(num_samples, seed=0):\n",
        "    np.random.seed(seed)\n",
        "    x = np.random.rand(num_samples, 10)  # 10 features\n",
        "    y = np.random.randint(0, 2, num_samples)  # Binary classification target\n",
        "    return pd.DataFrame(x, columns=[f'feature_{i}' for i in range(10)]), pd.Series(y, name='target')"
      ],
      "metadata": {
        "id": "a5rYaii1-9d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data for two hospitals\n",
        "hospital_A_data, hospital_A_labels = generate_synthetic_data(5, seed=1)\n",
        "hospital_B_data, hospital_B_labels = generate_synthetic_data(5, seed=2)"
      ],
      "metadata": {
        "id": "N4Max1eJ_FtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(hospital_A_data, hospital_A_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMhBZQwn_HXd",
        "outputId": "157ae895-4d7a-4cc5-fc15-6ee1efdaf646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
            "0   0.417022   0.720324   0.000114   0.302333   0.146756   0.092339   \n",
            "1   0.419195   0.685220   0.204452   0.878117   0.027388   0.670468   \n",
            "2   0.800745   0.968262   0.313424   0.692323   0.876389   0.894607   \n",
            "3   0.098347   0.421108   0.957890   0.533165   0.691877   0.315516   \n",
            "4   0.988861   0.748166   0.280444   0.789279   0.103226   0.447894   \n",
            "\n",
            "   feature_6  feature_7  feature_8  feature_9  \n",
            "0   0.186260   0.345561   0.396767   0.538817  \n",
            "1   0.417305   0.558690   0.140387   0.198101  \n",
            "2   0.085044   0.039055   0.169830   0.878143  \n",
            "3   0.686501   0.834626   0.018288   0.750144  \n",
            "4   0.908596   0.293614   0.287775   0.130029   0    1\n",
            "1    1\n",
            "2    1\n",
            "3    0\n",
            "4    0\n",
            "Name: target, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine data as if it were centralized (Normal Data)\n",
        "centralized_data = pd.concat([hospital_A_data, hospital_B_data])\n",
        "centralized_labels = pd.concat([hospital_A_labels, hospital_B_labels])\n",
        "\n",
        "# Display centralized data (Normal Data)\n",
        "print(\"Centralized Data (Normal Data):\")\n",
        "print(pd.concat([centralized_data, centralized_labels], axis=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzlVVdGdjtim",
        "outputId": "7eeab407-13f3-4616-b3fa-42408f6e3716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Centralized Data (Normal Data):\n",
            "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
            "0   0.417022   0.720324   0.000114   0.302333   0.146756   0.092339   \n",
            "1   0.419195   0.685220   0.204452   0.878117   0.027388   0.670468   \n",
            "2   0.800745   0.968262   0.313424   0.692323   0.876389   0.894607   \n",
            "3   0.098347   0.421108   0.957890   0.533165   0.691877   0.315516   \n",
            "4   0.988861   0.748166   0.280444   0.789279   0.103226   0.447894   \n",
            "0   0.435995   0.025926   0.549662   0.435322   0.420368   0.330335   \n",
            "1   0.621134   0.529142   0.134580   0.513578   0.184440   0.785335   \n",
            "2   0.505246   0.065287   0.428122   0.096531   0.127160   0.596745   \n",
            "3   0.467787   0.201743   0.640407   0.483070   0.505237   0.386893   \n",
            "4   0.964551   0.500008   0.889520   0.341614   0.567144   0.427546   \n",
            "\n",
            "   feature_6  feature_7  feature_8  feature_9  target  \n",
            "0   0.186260   0.345561   0.396767   0.538817       1  \n",
            "1   0.417305   0.558690   0.140387   0.198101       1  \n",
            "2   0.085044   0.039055   0.169830   0.878143       1  \n",
            "3   0.686501   0.834626   0.018288   0.750144       0  \n",
            "4   0.908596   0.293614   0.287775   0.130029       0  \n",
            "0   0.204649   0.619271   0.299655   0.266827       0  \n",
            "1   0.853975   0.494237   0.846561   0.079645       1  \n",
            "2   0.226012   0.106946   0.220306   0.349826       0  \n",
            "3   0.793637   0.580004   0.162299   0.700752       0  \n",
            "4   0.436747   0.776559   0.535604   0.953742       1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorFlow Datasets for Federated Learning\n",
        "#This function converts the input data (features and labels) into a tf.data.Dataset object. This conversion allows TensorFlow to manage data loading and batching efficiently.\n",
        "hospital_A_dataset = tf.data.Dataset.from_tensor_slices((hospital_A_data.values, hospital_A_labels.values)).batch(2)\n",
        "hospital_B_dataset = tf.data.Dataset.from_tensor_slices((hospital_B_data.values, hospital_B_labels.values)).batch(2)\n",
        "\n",
        "# Federated data is represented as a list of local datasets\n",
        "federated_data = [hospital_A_dataset, hospital_B_dataset]"
      ],
      "metadata": {
        "id": "ALejkN5M_V1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display federated data samples to demonstrate privacy\n",
        "# The list federated_data is used as input to the federated learning process. It represents the decentralized data stored on each client.\n",
        "print(\"\\nFederated Data (Data is kept locally):\")\n",
        "print(\"Hospital A Data Sample:\")\n",
        "for x, y in hospital_A_dataset.take(1):\n",
        "    print(f\"Features: {x.numpy()}, Labels: {y.numpy()}\")\n",
        "\n",
        "print(\"\\nHospital B Data Sample:\")\n",
        "for x, y in hospital_B_dataset.take(1):\n",
        "    print(f\"Features: {x.numpy()}, Labels: {y.numpy()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85tgfG6c_dgV",
        "outputId": "5b9733ae-e3db-4356-8958-3a8a116eda37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Federated Data (Data is kept locally):\n",
            "Hospital A Data Sample:\n",
            "Features: [[4.17022005e-01 7.20324493e-01 1.14374817e-04 3.02332573e-01\n",
            "  1.46755891e-01 9.23385948e-02 1.86260211e-01 3.45560727e-01\n",
            "  3.96767474e-01 5.38816734e-01]\n",
            " [4.19194514e-01 6.85219500e-01 2.04452250e-01 8.78117436e-01\n",
            "  2.73875932e-02 6.70467510e-01 4.17304802e-01 5.58689828e-01\n",
            "  1.40386939e-01 1.98101489e-01]], Labels: [1 1]\n",
            "\n",
            "Hospital B Data Sample:\n",
            "Features: [[0.4359949  0.02592623 0.54966248 0.43532239 0.4203678  0.33033482\n",
            "  0.20464863 0.61927097 0.29965467 0.26682728]\n",
            " [0.62113383 0.52914209 0.13457995 0.51357812 0.18443987 0.78533515\n",
            "  0.85397529 0.49423684 0.84656149 0.07964548]], Labels: [0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a model function using TFF's tff.learning.from_keras_model\n",
        "# federated learning environment using TensorFlow Federated (TFF).\n",
        "# model_fn: This function defines the machine learning model that will be used for federated learning.\n",
        "# The model_fn function provides TFF with a blueprint for creating the model on each client.\n",
        "# This function is called by TFF for every client participating in federated learning to ensure that the same model architecture is used across all clients.\n",
        "\n",
        "def model_fn():\n",
        "    keras_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(10,)),\n",
        "        tf.keras.layers.Dense(10, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    # This function converts the Keras model (keras_model) into a format that TensorFlow Federated (TFF) can use for federated learning.\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=hospital_A_dataset.element_spec, # input_spec: Specifies the structure of the input data (e.g., shape and data type).\n",
        "                                                    # This is required for TFF to understand the format of the data coming from each client.\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    )"
      ],
      "metadata": {
        "id": "HNwjAz9L_tLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a federated training process using the new API\n",
        "# tff.learning.algorithms.build_weighted_fed_avg: This function builds a federated training process using the Federated Averaging (FedAvg) algorithm.\n",
        "# FedAvg is one of the most common algorithms for federated learning, where each client computes updates to the model parameters locally,\n",
        "# and a central server averages these updates to improve the global model.\n",
        "# This line sets up the entire federated learning process. It defines how the model will be trained across multiple clients and how the local\n",
        "# model updates will be aggregated on the server.\n",
        "\n",
        "trainer = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    model_fn=model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.01)   #Specifies the optimizer to use on each client for local training\n",
        ")\n",
        "\n",
        "# Initialize the federated learning process\n",
        "# This function initializes the federated learning process and returns the initial state of the federated computation.\n",
        "# The state includes the initial model parameters (weights and biases) and any other information needed to begin the training process.\n",
        "state = trainer.initialize()\n",
        "\n",
        "# Simulate multiple rounds of federated training\n",
        "# Each round consists of local training on each client followed by aggregation of updates on the server.\n",
        "# The number of rounds (NUM_ROUNDS) determines how many times this process will repeat.\n",
        "NUM_ROUNDS = 100 #This line sets the number of training rounds (iterations) for federated learning.\n",
        "\n",
        "\n",
        "for round_num in range(1, NUM_ROUNDS + 1):\n",
        "    state, metrics = trainer.next(state, federated_data)  #federated_data: The list of local datasets for each client (e.g., Hospital A and Hospital B).\n",
        "                                                          #  This data is used by each client to perform local training.\n",
        "    print(f'Round {round_num}, Metrics: {metrics}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0MkZSfG_lvS",
        "outputId": "547cf8e8-d0b0-458a-ef84-6b2ad97414d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.8199951), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 2, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.81347895), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 3, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.8083316), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 4, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.8031208), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 5, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.79812986), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 6, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.79334575), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 7, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7887564), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 8, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7845963), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 9, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7805726), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 10, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7767088), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 11, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.77299577), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 12, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7699339), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 13, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7669311), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 14, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7640335), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 15, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7612457), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 16, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.75854737), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 17, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7559432), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 18, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.753432), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 19, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.75099313), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 20, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.74864215), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 21, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7463565), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 22, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7441595), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 23, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7420751), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 24, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7400489), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 25, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7378978), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 26, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7358157), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 27, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.73378503), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 28, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7318174), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 29, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7298962), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 30, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.72803265), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 31, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7262111), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 32, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7244954), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 33, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.7230095), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 34, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.72156346), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 35, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.7202746), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 36, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.71915615), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 37, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.7181121), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 38, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.7170483), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 39, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.71604407), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 40, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.71503353), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 41, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.71406347), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 42, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.71310514), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 43, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.71216404), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 44, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.711257), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 45, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7103399), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 46, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7094833), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 47, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7085885), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 48, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7077742), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 49, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.70691854), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 50, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7061175), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 51, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.70531094), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 52, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7045194), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 53, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.70376146), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 54, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7029835), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 55, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7022615), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 56, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7013997), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 57, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.7005834), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 58, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6997843), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 59, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.69897175), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 60, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.2), ('loss', 0.6982094), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 61, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.69723254), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 62, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6963748), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 63, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6955528), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 64, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6948183), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 65, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.69432104), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 66, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.69379246), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 67, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.69326544), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 68, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6927838), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 69, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6922568), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 70, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.6917405), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 71, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.3), ('loss', 0.691154), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 72, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.69055927), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 73, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.6900093), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 74, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.6894313), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 75, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.68886006), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 76, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.68832064), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 77, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.6877532), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 78, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.687211), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 79, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.6866784), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 80, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.68611765), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 81, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.68561435), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 82, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.68507576), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 83, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.68453276), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 84, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.6840474), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 85, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.4), ('loss', 0.683545), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 86, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.6831541), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 87, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.682775), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 88, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.6823727), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 89, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.5), ('loss', 0.6819971), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 90, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.68161297), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 91, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.6812717), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 92, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.68096846), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 93, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.68064296), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 94, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.68030393), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 95, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.6800188), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 96, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.67968506), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 97, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.6793603), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 98, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.6790826), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 99, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.67874706), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n",
            "Round 100, Metrics: OrderedDict([('distributor', ()), ('client_work', OrderedDict([('train', OrderedDict([('binary_accuracy', 0.6), ('loss', 0.6784443), ('num_examples', 10), ('num_batches', 6)]))])), ('aggregator', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('finalizer', OrderedDict([('update_non_finite', 0)]))])\n"
          ]
        }
      ]
    }
  ]
}