{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Hereâ€™s a simple example of synchronous distributed training using TensorFlow and Horovod:"
      ],
      "metadata": {
        "id": "8nMoNshacEZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synchronization Process Explained \\\\\n",
        "**Gradient Calculation: **Each worker computes gradients on its local batch of data. \\\\\n",
        "**Gradient Averaging:** Horovod's Allreduce operation averages gradients from all workers. \\\\\n",
        "**Model Update:** Each worker updates its model parameters with the averaged gradients. \\\\\n",
        "**Broadcasting Initial Variables:** The initial model parameters are broadcasted from rank 0 to ensure all workers start with the same model parameters. \\\\"
      ],
      "metadata": {
        "id": "eyDtSAEodz8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install horovod"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bF-bui2cbBs",
        "outputId": "d09a0cc3-4170-4fa2-c21b-7e1948307553"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting horovod\n",
            "  Using cached horovod-0.28.1.tar.gz (3.5 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from horovod) (2.2.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from horovod) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from horovod) (6.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from horovod) (24.1)\n",
            "Requirement already satisfied: cffi>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from horovod) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.0->horovod) (2.22)\n",
            "Building wheels for collected packages: horovod\n",
            "  Building wheel for horovod (setup.py) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Cn4aRgEtkbDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Set up the strategy for distributed training\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "print(f\"Number of devices: {strategy.num_replicas_in_sync}\")\n",
        "\n",
        "# Open a strategy scope\n",
        "with strategy.scope():\n",
        "    # Define a simple model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Define optimizer\n",
        "    opt = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Scale batch size based on the number of devices\n",
        "batch_size = 128 * strategy.num_replicas_in_sync\n",
        "\n",
        "# Create dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset, epochs=5)\n"
      ],
      "metadata": {
        "id": "vFogAQHojbrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "C0jXYCw3bgxF",
        "outputId": "cc755d1c-e409-4a72-b021-1a7a2391fb39"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'horovod'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7c8f7497c909>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhorovod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Initialize Horovod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'horovod'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import horovod.tensorflow as hvd\n",
        "\n",
        "# Initialize Horovod\n",
        "hvd.init()\n",
        "\n",
        "# Pin GPU to be used by each process\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
        "\n",
        "# Define a simple model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Horovod: adjust learning rate based on the number of GPUs\n",
        "opt = tf.keras.optimizers.Adam(0.001 * hvd.size())\n",
        "\n",
        "# Horovod: add Horovod DistributedOptimizer\n",
        "opt = hvd.DistributedOptimizer(opt)\n",
        "\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Horovod: use `hvd.size()` to scale batch size\n",
        "batch_size = 128 * hvd.size()\n",
        "\n",
        "# Horovod: use `hvd.rank()`, `hvd.size()` to partition data\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shard(hvd.size(), hvd.rank()).shuffle(10000).batch(batch_size)\n",
        "\n",
        "callbacks = [\n",
        "    # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
        "    # This is necessary to ensure consistent initialization of all workers when\n",
        "    # training is started with random weights or restored from a checkpoint.\n",
        "    hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
        "]\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset, epochs=5, callbacks=callbacks)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import horovod.tensorflow as hvd\n",
        "\n",
        "# Initialize Horovod\n",
        "hvd.init()\n",
        "\n",
        "# Pin GPU to be used by each worker\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')\n",
        "\n",
        "# Build a simple model\n",
        "def create_model():\n",
        "    return tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "# Each worker creates a model instance\n",
        "model = create_model()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "# Wrap the optimizer with Horovod DistributedOptimizer\n",
        "optimizer = hvd.DistributedOptimizer(optimizer)\n",
        "\n",
        "# Compile the model with the wrapped optimizer\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Horovod: Adjust batch size to be larger in distributed settings\n",
        "batch_size = 128 * hvd.size()\n",
        "\n",
        "# Create a dataset and shard it among workers\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shard(hvd.size(), hvd.rank()).shuffle(10000).batch(batch_size)\n",
        "\n",
        "# Create a callback to broadcast initial variable states from rank 0 to all other processes.\n",
        "callbacks = [\n",
        "    hvd.callbacks.BroadcastGlobalVariablesCallback(0)\n",
        "]\n",
        "\n",
        "# Optionally, adjust learning rate based on the number of workers (warmup).\n",
        "initial_lr = 0.001\n",
        "scaled_lr = initial_lr * hvd.size()\n",
        "optimizer.learning_rate = scaled_lr\n",
        "\n",
        "# Train the model\n",
        "model.fit(dataset, epochs=5, callbacks=callbacks)\n",
        "\n",
        "# Save model only on worker 0\n",
        "if hvd.rank() == 0:\n",
        "    model.save('my_model.h5')\n"
      ],
      "metadata": {
        "id": "_kg7b452cbq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread, Lock\n",
        "import numpy as np\n",
        "\n",
        "# Function to simulate gradient computation\n",
        "def compute_gradients(data, model_parameters):\n",
        "    # A simple gradient computation example: gradient is proportional to model parameters\n",
        "    gradients = model_parameters - data  # Example: dummy gradient calculation\n",
        "    return gradients\n",
        "\n",
        "# Number of epochs and learning rate\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Number of workers (threads)\n",
        "num_workers = 4\n",
        "\n",
        "# Simulated data for each worker\n",
        "# Each worker gets a random data point for simplicity\n",
        "data = [np.random.randn(10) for _ in range(num_workers)]\n",
        "\n",
        "# Shared model parameters (initialization)\n",
        "model_parameters = np.random.randn(10)\n",
        "\n",
        "# Lock for thread-safe updates\n",
        "lock = Lock()\n",
        "\n",
        "def worker(worker_id, data_point):\n",
        "    global model_parameters\n",
        "    for epoch in range(epochs):\n",
        "        # Compute gradients on the local subset of data\n",
        "        gradients = compute_gradients(data_point, model_parameters)\n",
        "\n",
        "        # Asynchronously update global model parameters\n",
        "        with lock:  # Ensure thread-safe updates\n",
        "            model_parameters -= learning_rate * gradients\n",
        "\n",
        "        print(f\"Worker {worker_id} updated model parameters in epoch {epoch}.\")\n",
        "\n",
        "# Create threads for each worker\n",
        "workers = [Thread(target=worker, args=(i, data[i])) for i in range(num_workers)]\n",
        "\n",
        "# Start all workers\n",
        "for w in workers:\n",
        "    w.start()\n",
        "\n",
        "# Wait for all workers to finish\n",
        "for w in workers:\n",
        "    w.join()\n",
        "\n",
        "# Print the final model parameters\n",
        "print(\"Final model parameters:\", model_parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu1_8YysfGSP",
        "outputId": "e13bc99e-ee61-4e18-dd09-30b8daa3f10b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 updated model parameters in epoch 0.Worker 1 updated model parameters in epoch 0.\n",
            "Worker 1 updated model parameters in epoch 1.\n",
            "Worker 0 updated model parameters in epoch 1.\n",
            "Worker 0 updated model parameters in epoch 2.\n",
            "Worker 0 updated model parameters in epoch 3.\n",
            "Worker 0 updated model parameters in epoch 4.\n",
            "Worker 0 updated model parameters in epoch 5.\n",
            "Worker 0 updated model parameters in epoch 6.\n",
            "Worker 0 updated model parameters in epoch 7.\n",
            "Worker 0 updated model parameters in epoch 8.\n",
            "Worker 0 updated model parameters in epoch 9.\n",
            "\n",
            "Worker 1 updated model parameters in epoch 2.\n",
            "Worker 1 updated model parameters in epoch 3.\n",
            "Worker 1 updated model parameters in epoch 4.\n",
            "Worker 1 updated model parameters in epoch 5.\n",
            "Worker 1 updated model parameters in epoch 6.\n",
            "Worker 1 updated model parameters in epoch 7.\n",
            "Worker 1 updated model parameters in epoch 8.\n",
            "Worker 1 updated model parameters in epoch 9.\n",
            "Worker 2 updated model parameters in epoch 0.\n",
            "Worker 2 updated model parameters in epoch 1.\n",
            "Worker 2 updated model parameters in epoch 2.\n",
            "Worker 2 updated model parameters in epoch 3.\n",
            "Worker 2 updated model parameters in epoch 4.\n",
            "Worker 2 updated model parameters in epoch 5.\n",
            "Worker 2 updated model parameters in epoch 6.\n",
            "Worker 2 updated model parameters in epoch 7.\n",
            "Worker 2 updated model parameters in epoch 8.\n",
            "Worker 2 updated model parameters in epoch 9.\n",
            "Worker 3 updated model parameters in epoch 0.\n",
            "Worker 3 updated model parameters in epoch 1.\n",
            "Worker 3 updated model parameters in epoch 2.\n",
            "Worker 3 updated model parameters in epoch 3.\n",
            "Worker 3 updated model parameters in epoch 4.\n",
            "Worker 3 updated model parameters in epoch 5.\n",
            "Worker 3 updated model parameters in epoch 6.\n",
            "Worker 3 updated model parameters in epoch 7.\n",
            "Worker 3 updated model parameters in epoch 8.\n",
            "Worker 3 updated model parameters in epoch 9.\n",
            "Final model parameters: [-0.50472016  0.59394869 -1.1084889  -0.7388325  -0.38951287 -1.46829343\n",
            " -0.32793454 -1.4050265   0.02007987 -0.57381503]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decentralized SGD"
      ],
      "metadata": {
        "id": "bkQampvakdrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from threading import Thread, Lock\n",
        "import random\n",
        "\n",
        "# Initialize parameters for each worker\n",
        "num_workers = 4\n",
        "epochs = 20\n",
        "learning_rate = 0.01\n",
        "exchange_interval = 5  # Number of epochs after which parameter exchange occurs\n",
        "\n",
        "# Dummy data for each worker\n",
        "data = [np.random.randn(10) for _ in range(num_workers)]\n",
        "\n",
        "# Initialize parameters for each worker (each worker starts with different parameters)\n",
        "worker_params = [np.random.randn(10) for _ in range(num_workers)]\n",
        "\n",
        "# Lock for thread-safe updates\n",
        "lock = Lock()\n",
        "\n",
        "def compute_gradients(data, params):\n",
        "    \"\"\"\n",
        "    Simulated gradient computation.\n",
        "    In practice, this would be computed based on the model's loss with respect to the data.\n",
        "    \"\"\"\n",
        "    # Example gradient: simple negative gradient proportional to the current parameters\n",
        "    gradients = params - data\n",
        "    return gradients\n",
        "\n",
        "def get_neighbors(worker_id):\n",
        "    \"\"\"\n",
        "    Determine the neighbors for a given worker.\n",
        "    This function simulates a simple ring topology where each worker exchanges parameters with its next neighbor.\n",
        "    \"\"\"\n",
        "    if worker_id == 0:\n",
        "        return [1, num_workers - 1]  # First worker connects to the next and the last\n",
        "    elif worker_id == num_workers - 1:\n",
        "        return [0, num_workers - 2]  # Last worker connects to the first and the previous\n",
        "    else:\n",
        "        return [worker_id - 1, worker_id + 1]  # Middle workers connect to their neighbors\n",
        "\n",
        "def decentralized_update(worker_id, data_point):\n",
        "    global worker_params\n",
        "    for epoch in range(epochs):\n",
        "        # Compute gradients based on local parameters and data\n",
        "        local_gradients = compute_gradients(data_point, worker_params[worker_id])\n",
        "\n",
        "        # Update local parameters\n",
        "        with lock:  # Ensure thread-safe update\n",
        "            worker_params[worker_id] -= learning_rate * local_gradients\n",
        "\n",
        "        # Periodically exchange parameters with neighbors\n",
        "        if epoch % exchange_interval == 0:\n",
        "            neighbors = get_neighbors(worker_id)\n",
        "            with lock:  # Ensure thread-safe parameter exchange\n",
        "                for neighbor in neighbors:\n",
        "                    # Average parameters with neighbor\n",
        "                    worker_params[worker_id] = (worker_params[worker_id] + worker_params[neighbor]) / 2\n",
        "\n",
        "            print(f\"Worker {worker_id} exchanged parameters with neighbors: {neighbors} at epoch {epoch}.\")\n",
        "\n",
        "        print(f\"Worker {worker_id} updated parameters at epoch {epoch}.\")\n"
      ],
      "metadata": {
        "id": "40Q0Zl5rkf7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and start threads for each worker\n",
        "threads = [Thread(target=decentralized_update, args=(i, data[i])) for i in range(num_workers)]\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "# Final output of parameters for all workers\n",
        "print(\"\\nFinal parameters of all workers:\")\n",
        "for worker_id in range(num_workers):\n",
        "    print(f\"Worker {worker_id} final parameters: {worker_params[worker_id]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6blZrQfkkGs",
        "outputId": "c3a83053-4101-4c63-e6e7-4babb4ce9b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 exchanged parameters with neighbors: [1, 3] at epoch 0.Worker 1 exchanged parameters with neighbors: [0, 2] at epoch 0.\n",
            "Worker 1 updated parameters at epoch 0.\n",
            "Worker 1 updated parameters at epoch 1.\n",
            "Worker 1 updated parameters at epoch 2.\n",
            "Worker 1 updated parameters at epoch 3.\n",
            "Worker 2 exchanged parameters with neighbors: [1, 3] at epoch 0.\n",
            "Worker 2 updated parameters at epoch 0.\n",
            "Worker 2 updated parameters at epoch 1.\n",
            "Worker 2 updated parameters at epoch 2.\n",
            "Worker 2 updated parameters at epoch 3.\n",
            "Worker 2 updated parameters at epoch 4.\n",
            "Worker 2 exchanged parameters with neighbors: [1, 3] at epoch 5.\n",
            "Worker 2 updated parameters at epoch 5.\n",
            "Worker 2 updated parameters at epoch 6.\n",
            "Worker 2 updated parameters at epoch 7.\n",
            "Worker 2 updated parameters at epoch 8.\n",
            "Worker 2 updated parameters at epoch 9.\n",
            "Worker 2 exchanged parameters with neighbors: [1, 3] at epoch 10.\n",
            "Worker 2 updated parameters at epoch 10.\n",
            "Worker 2 updated parameters at epoch 11.\n",
            "Worker 2 updated parameters at epoch 12.\n",
            "Worker 2 updated parameters at epoch 13.\n",
            "Worker 2 updated parameters at epoch 14.\n",
            "Worker 2 exchanged parameters with neighbors: [1, 3] at epoch 15.\n",
            "Worker 2 updated parameters at epoch 15.\n",
            "Worker 2 updated parameters at epoch 16.\n",
            "Worker 2 updated parameters at epoch 17.\n",
            "Worker 2 updated parameters at epoch 18.\n",
            "Worker 2 updated parameters at epoch 19.\n",
            "\n",
            "Worker 0 updated parameters at epoch 0.\n",
            "Worker 0 updated parameters at epoch 1.\n",
            "Worker 0 updated parameters at epoch 2.\n",
            "Worker 0 updated parameters at epoch 3.\n",
            "Worker 0 updated parameters at epoch 4.\n",
            "Worker 0 exchanged parameters with neighbors: [1, 3] at epoch 5.\n",
            "Worker 0 updated parameters at epoch 5.\n",
            "Worker 0 updated parameters at epoch 6.\n",
            "Worker 0 updated parameters at epoch 7.\n",
            "Worker 0 updated parameters at epoch 8.\n",
            "Worker 0 updated parameters at epoch 9.\n",
            "Worker 0 exchanged parameters with neighbors: [1, 3] at epoch 10.\n",
            "Worker 0 updated parameters at epoch 10.\n",
            "Worker 0 updated parameters at epoch 11.\n",
            "Worker 0 updated parameters at epoch 12.\n",
            "Worker 0 updated parameters at epoch 13.\n",
            "Worker 0 updated parameters at epoch 14.\n",
            "Worker 0 exchanged parameters with neighbors: [1, 3] at epoch 15.\n",
            "Worker 0 updated parameters at epoch 15.\n",
            "Worker 0 updated parameters at epoch 16.\n",
            "Worker 0 updated parameters at epoch 17.\n",
            "Worker 0 updated parameters at epoch 18.\n",
            "Worker 0 updated parameters at epoch 19.\n",
            "Worker 1 updated parameters at epoch 4.\n",
            "Worker 1 exchanged parameters with neighbors: [0, 2] at epoch 5.\n",
            "Worker 1 updated parameters at epoch 5.\n",
            "Worker 1 updated parameters at epoch 6.\n",
            "Worker 3 exchanged parameters with neighbors: [0, 2] at epoch 0.Worker 1 updated parameters at epoch 7.\n",
            "Worker 1 updated parameters at epoch 8.\n",
            "Worker 1 updated parameters at epoch 9.\n",
            "Worker 1 exchanged parameters with neighbors: [0, 2] at epoch 10.\n",
            "Worker 1 updated parameters at epoch 10.\n",
            "Worker 1 updated parameters at epoch 11.\n",
            "Worker 1 updated parameters at epoch 12.\n",
            "Worker 1 updated parameters at epoch 13.\n",
            "Worker 1 updated parameters at epoch 14.\n",
            "Worker 1 exchanged parameters with neighbors: [0, 2] at epoch 15.\n",
            "Worker 1 updated parameters at epoch 15.\n",
            "Worker 1 updated parameters at epoch 16.\n",
            "Worker 1 updated parameters at epoch 17.\n",
            "Worker 1 updated parameters at epoch 18.\n",
            "Worker 1 updated parameters at epoch 19.\n",
            "\n",
            "Worker 3 updated parameters at epoch 0.\n",
            "Worker 3 updated parameters at epoch 1.\n",
            "Worker 3 updated parameters at epoch 2.\n",
            "Worker 3 updated parameters at epoch 3.\n",
            "Worker 3 updated parameters at epoch 4.\n",
            "Worker 3 exchanged parameters with neighbors: [0, 2] at epoch 5.\n",
            "Worker 3 updated parameters at epoch 5.\n",
            "Worker 3 updated parameters at epoch 6.\n",
            "Worker 3 updated parameters at epoch 7.\n",
            "Worker 3 updated parameters at epoch 8.\n",
            "Worker 3 updated parameters at epoch 9.\n",
            "Worker 3 exchanged parameters with neighbors: [0, 2] at epoch 10.\n",
            "Worker 3 updated parameters at epoch 10.\n",
            "Worker 3 updated parameters at epoch 11.\n",
            "Worker 3 updated parameters at epoch 12.\n",
            "Worker 3 updated parameters at epoch 13.\n",
            "Worker 3 updated parameters at epoch 14.\n",
            "Worker 3 exchanged parameters with neighbors: [0, 2] at epoch 15.\n",
            "Worker 3 updated parameters at epoch 15.\n",
            "Worker 3 updated parameters at epoch 16.\n",
            "Worker 3 updated parameters at epoch 17.\n",
            "Worker 3 updated parameters at epoch 18.\n",
            "Worker 3 updated parameters at epoch 19.\n",
            "\n",
            "Final parameters of all workers:\n",
            "Worker 0 final parameters: [ 0.0566675  -0.1303388   0.52768127 -0.67919418  0.63013816 -0.50418342\n",
            " -0.07309428 -0.00766843 -0.81290136  0.25286903]\n",
            "Worker 1 final parameters: [ 0.03889946 -0.08305911  0.49798835 -0.70082537  0.46415954 -0.5093405\n",
            " -0.10211485  0.00655586 -0.74188679  0.1676575 ]\n",
            "Worker 2 final parameters: [ 0.06753782 -0.15379528  0.41687012 -0.70350741  0.58342491 -0.56648569\n",
            " -0.10790745 -0.01052656 -0.82487415  0.28749088]\n",
            "Worker 3 final parameters: [ 0.05900736 -0.12379402  0.32028098 -0.61639586  0.56148209 -0.63865181\n",
            " -0.04071707 -0.02731217 -0.84114171  0.2971076 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Distributed SGD with Gradient Quantization"
      ],
      "metadata": {
        "id": "UMAOHvM6l7bC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from threading import Thread, Lock\n",
        "\n",
        "# Number of workers\n",
        "num_workers = 4\n",
        "epochs = 20\n",
        "learning_rate = 0.01\n",
        "bits = 8  # Number of bits for quantization\n",
        "\n",
        "# Dummy data for each worker\n",
        "data = [np.random.randn(10) for _ in range(num_workers)]\n",
        "\n",
        "# Shared model parameters (initialization)\n",
        "model_parameters = np.random.randn(10)\n",
        "\n",
        "# Lock for thread-safe updates\n",
        "lock = Lock()\n",
        "\n",
        "def compute_gradients(data, params):\n",
        "    \"\"\"\n",
        "    Simulated gradient computation.\n",
        "    In practice, this would be computed based on the model's loss with respect to the data.\n",
        "    \"\"\"\n",
        "    # Example gradient: simple negative gradient proportional to the current parameters\n",
        "    gradients = params - data\n",
        "    return gradients\n",
        "\n",
        "def quantize_gradients(gradients, bits=8):\n",
        "    \"\"\"\n",
        "    Quantize gradients to reduce precision to a specific number of bits.\n",
        "    \"\"\"\n",
        "    max_val = np.max(np.abs(gradients))\n",
        "    scale = (2 ** bits - 1) / max_val\n",
        "    quantized = np.round(gradients * scale) / scale\n",
        "    return quantized\n",
        "\n",
        "def worker_with_quantization(worker_id, data_point):\n",
        "    global model_parameters\n",
        "    for epoch in range(epochs):\n",
        "        # Compute gradients\n",
        "        gradients = compute_gradients(data_point, model_parameters)\n",
        "\n",
        "        # Quantize gradients\n",
        "        quantized_gradients = quantize_gradients(gradients, bits)\n",
        "\n",
        "        # Update parameters with quantized gradients\n",
        "        with lock:\n",
        "            model_parameters -= learning_rate * quantized_gradients\n",
        "\n",
        "        print(f\"Worker {worker_id} updated model parameters with quantized gradients at epoch {epoch}.\")\n",
        "\n",
        "# Create and start threads for each worker\n",
        "threads = [Thread(target=worker_with_quantization, args=(i, data[i])) for i in range(num_workers)]\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "# Print the final model parameters\n",
        "print(\"\\nFinal model parameters:\", model_parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Iwbmthil7yr",
        "outputId": "ca4c0967-139e-43ee-f695-3467c4f72a0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 4.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 5.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 6.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 7.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 8.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 9.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 10.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 11.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 12.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 13.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 14.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 15.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 1.Worker 0 updated model parameters with quantized gradients at epoch 16.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 17.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 18.\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 19.\n",
            "\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 4.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 5.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 6.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 7.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 8.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 9.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 10.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 11.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 12.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 13.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 14.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 15.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 16.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 17.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 18.\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 19.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 4.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 5.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 6.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 7.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 8.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 9.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 10.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 11.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 12.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 13.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 14.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 15.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 16.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 17.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 18.\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 19.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 4.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 5.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 6.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 7.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 8.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 9.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 10.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 11.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 12.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 13.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 14.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 15.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 16.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 17.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 18.\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 19.\n",
            "\n",
            "Final model parameters: [-0.15208487  0.44228307  0.14425512 -0.02941317  0.4979465   0.561097\n",
            " -0.30166659 -0.95801978 -0.62307721 -0.04491007]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from threading import Thread, Lock\n",
        "\n",
        "# Number of workers\n",
        "num_workers = 4\n",
        "epochs = 5\n",
        "learning_rate = 0.01\n",
        "bits = 8  # Number of bits for quantization\n",
        "\n",
        "# Dummy data for each worker\n",
        "data = [np.random.randn(10) for _ in range(num_workers)]\n",
        "\n",
        "# Shared model parameters (initialization)\n",
        "model_parameters = np.random.randn(10)\n",
        "\n",
        "# Lock for thread-safe updates\n",
        "lock = Lock()\n",
        "\n",
        "def compute_gradients(data, params):\n",
        "    \"\"\"\n",
        "    Simulated gradient computation.\n",
        "    In practice, this would be computed based on the model's loss with respect to the data.\n",
        "    \"\"\"\n",
        "    # Example gradient: simple negative gradient proportional to the current parameters\n",
        "    gradients = params - data\n",
        "    return gradients\n",
        "\n",
        "def quantize_gradients(gradients, bits=8):\n",
        "    \"\"\"\n",
        "    Quantize gradients to reduce precision to a specific number of bits.\n",
        "    \"\"\"\n",
        "    max_val = np.max(np.abs(gradients))\n",
        "    scale = (2 ** bits - 1) / max_val\n",
        "    quantized = np.round(gradients * scale) / scale\n",
        "    return quantized\n",
        "\n",
        "def worker_with_quantization(worker_id, data_point):\n",
        "    global model_parameters\n",
        "    for epoch in range(epochs):\n",
        "        # Compute gradients\n",
        "        gradients = compute_gradients(data_point, model_parameters)\n",
        "\n",
        "        # Quantize gradients\n",
        "        quantized_gradients = quantize_gradients(gradients, bits)\n",
        "\n",
        "        # Print gradients before and after quantization\n",
        "        print(f\"Worker {worker_id}, Epoch {epoch}:\")\n",
        "        print(f\"  Gradients before quantization: {gradients}\")\n",
        "        print(f\"  Gradients after quantization:  {quantized_gradients}\\n\")\n",
        "\n",
        "        # Update parameters with quantized gradients\n",
        "        with lock:\n",
        "            model_parameters -= learning_rate * quantized_gradients\n",
        "\n",
        "        print(f\"Worker {worker_id} updated model parameters with quantized gradients at epoch {epoch}.\")\n",
        "\n",
        "# Create and start threads for each worker\n",
        "threads = [Thread(target=worker_with_quantization, args=(i, data[i])) for i in range(num_workers)]\n",
        "\n",
        "for thread in threads:\n",
        "    thread.start()\n",
        "\n",
        "# Wait for all threads to finish\n",
        "for thread in threads:\n",
        "    thread.join()\n",
        "\n",
        "# Print the final model parameters\n",
        "print(\"\\nFinal model parameters:\", model_parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuVnWFzkmZUV",
        "outputId": "69386fea-3613-4743-f86c-769f846de8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker 0, Epoch 0:Worker 1, Epoch 0:\n",
            "  Gradients before quantization: [-0.28432148 -1.29229182  0.74060259 -1.67875105  1.65377276  0.50226756\n",
            "  1.06744535 -1.97821947 -0.26637252  1.54975347]\n",
            "  Gradients after quantization:  [-0.28703577 -1.29553981  0.73698373 -1.67566826  1.65239509  0.50425202\n",
            "  1.07056583 -1.97821947 -0.2637626   1.55154468]\n",
            "\n",
            "\n",
            "  Gradients before quantization: [-0.30390838 -1.63080329  1.36668098 -1.76386803  1.50498167  1.48528902\n",
            "  1.80408624 -1.05684439  0.40584963  0.42470923]\n",
            "  Gradients after quantization:  [-0.30421846 -1.63428989  1.36544566 -1.76163715  1.50694262  1.48571808\n",
            "  1.80408624 -1.05415235  0.40326634  0.42449088]\n",
            "\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 0, Epoch 1:\n",
            "  Gradients before quantization: [-0.3008662  -1.61446039  1.35302652 -1.74625166  1.48991224  1.47043184\n",
            "  1.78604538 -1.04630286  0.40181696  0.42046432]\n",
            "  Gradients after quantization:  [-0.30117628 -1.61794699  1.35179121 -1.74402078  1.4918732   1.4708609\n",
            "  1.78604538 -1.04361083  0.39923367  0.42024597]\n",
            "\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 0, Epoch 2:\n",
            "  Gradients before quantization: [-0.29785443 -1.59828092  1.33950861 -1.72881145  1.47499351  1.45572323\n",
            "  1.76818492 -1.03586676  0.39782463  0.41626186]\n",
            "  Gradients after quantization:  [-0.29816452 -1.59483346  1.3382733  -1.72658057  1.47695447  1.45615229\n",
            "  1.76818492 -1.03317472  0.39524134  0.41604351]\n",
            "\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 0, Epoch 3:\n",
            "  Gradients before quantization: [-0.29487279 -1.58233259  1.32612587 -1.71154564  1.46022397  1.4411617\n",
            "  1.75050308 -1.02553501  0.39387221  0.41210143]Worker 1 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 1, Epoch 1:\n",
            "  Gradients after quantization:  [-0.29518287 -1.58574984  1.32489056 -1.70931477  1.46218492  1.44159077\n",
            "  1.75050308 -1.02284297  0.39128892  0.41188308]\n",
            "\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 0, Epoch 4:\n",
            "  Gradients before quantization: [-0.2890506  -1.55351969  1.30550713 -1.67769581  1.42907817  1.42170328\n",
            "  1.72229239 -0.99552438  0.39259695  0.39246715]\n",
            "  Gradients after quantization:  [-0.29042577 -1.55344019  1.30353894 -1.67501377  1.43186661  1.41835844\n",
            "  1.72229239 -0.99285091  0.39173709  0.39173709]\n",
            "\n",
            "Worker 0 updated model parameters with quantized gradients at epoch 4.\n",
            "\n",
            "  Gradients before quantization: [-0.27241553 -1.23086572  0.69267765 -1.60967198  1.59249111  0.45309773\n",
            "  1.00315652 -1.9271279  -0.2757123   1.52163022]\n",
            "  Gradients after quantization:  [-0.27206512 -1.23185038  0.69527752 -1.6097186   1.59460387  0.45344186\n",
            "  1.00512945 -1.9271279  -0.27206512  1.51903023]\n",
            "\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 1, Epoch 2:\n",
            "  Gradients before quantization: [-0.26383879 -1.18715531  0.65944058 -1.55973151  1.54760455  0.41996382\n",
            "  0.95837727 -1.88769968 -0.28082191  1.49840372]\n",
            "  Gradients after quantization:  [-0.26649878 -1.18443902  0.6588442  -1.56197895  1.54717346  0.4219564\n",
            "  0.95495396 -1.88769968 -0.28130427  1.49535426]\n",
            "\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 1, Epoch 3:\n",
            "  Gradients before quantization: [-0.26117381 -1.17531092  0.65285214 -1.54411172  1.53213282  0.41574425\n",
            "  0.94882773 -1.86882268 -0.27800887  1.48345017]\n",
            "  Gradients after quantization:  [-0.26383379 -1.17259463  0.65225576 -1.54635916  1.53170173  0.41773684\n",
            "  0.94540442 -1.86882268 -0.27849122  1.48040071]\n",
            "\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 1, Epoch 4:\n",
            "  Gradients before quantization: [-0.25853547 -1.16358498  0.64632958 -1.52864813  1.5168158   0.41156689\n",
            "  0.93937369 -1.85013446 -0.27522396  1.46864617]\n",
            "  Gradients after quantization:  [-0.26119545 -1.16086868  0.6457332  -1.53089557  1.51638471  0.41355947\n",
            "  0.93595037 -1.85013446 -0.27570631  1.46559671]\n",
            "\n",
            "Worker 1 updated model parameters with quantized gradients at epoch 4.\n",
            "Worker 2, Epoch 0:\n",
            "  Gradients before quantization: [-2.50949969 -1.39628484 -0.48015958 -0.93295193  0.17581815  2.81445521\n",
            "  1.46214956 -0.54196392 -0.07239167  1.54059713]\n",
            "  Gradients after quantization:  [-2.50541699 -1.40170907 -0.48563149 -0.93815174  0.17659327  2.81445521\n",
            "  1.45689446 -0.54081688 -0.07725955  1.5451911 ]\n",
            "\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 2, Epoch 1:\n",
            "  Gradients before quantization: [-2.48444552 -1.38226775 -0.47530327 -0.92357041  0.17405222  2.78631066\n",
            "  1.44758062 -0.53655575 -0.07161908  1.52514522]\n",
            "  Gradients after quantization:  [-2.48036282 -1.38769198 -0.46984846 -0.92877022  0.17482734  2.78631066\n",
            "  1.44232552 -0.53540872 -0.07648696  1.52973919]\n",
            "\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 2, Epoch 2:\n",
            "  Gradients before quantization: [-2.45964189 -1.36839083 -0.47060478 -0.91428271  0.17230395  2.75844755\n",
            "  1.43315736 -0.53120166 -0.07085421  1.50984783]\n",
            "  Gradients after quantization:  [-2.45555919 -1.36299761 -0.47596742 -0.91948252  0.17307906  2.75844755\n",
            "  1.42790226 -0.53005463 -0.07572209  1.51444179]\n",
            "\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 2, Epoch 3:\n",
            "  Gradients before quantization: [-2.4350863  -1.35476085 -0.46584511 -0.90508788  0.17057316  2.73086308\n",
            "  1.41887834 -0.52590111 -0.07009698  1.49470341]\n",
            "  Gradients after quantization:  [-2.4310036  -1.36007691 -0.46049848 -0.91028769  0.17134827  2.73086308\n",
            "  1.41362324 -0.52475408 -0.07496487  1.49929738]\n",
            "\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 2, Epoch 4:\n",
            "  Gradients before quantization: [-2.41077626 -1.34116008 -0.46124012 -0.895985    0.16885967  2.70355445\n",
            "  1.40474211 -0.52065357 -0.06934734  1.47971043]\n",
            "  Gradients after quantization:  [-2.40669357 -1.33587396 -0.46649567 -0.90118482  0.16963479  2.70355445\n",
            "  1.39948701 -0.51950654 -0.07421522  1.4843044 ]\n",
            "\n",
            "Worker 2 updated model parameters with quantized gradients at epoch 4.\n",
            "Worker 3, Epoch 0:\n",
            "  Gradients before quantization: [ 0.98948668 -2.08820015 -0.65004074 -1.56836173 -0.43258901  2.03541092\n",
            "  0.51679406 -0.45474056 -1.02251152  0.4387995 ]\n",
            "  Gradients after quantization:  [ 0.99087144 -2.08820015 -0.64693259 -1.57229188 -0.43401807  2.03906603\n",
            "  0.51590827 -0.45858513 -1.02362752  0.44220709]\n",
            "\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 0.\n",
            "Worker 3, Epoch 1:\n",
            "  Gradients before quantization: [ 0.97957796 -2.06731815 -0.64357141 -1.55263881 -0.42824883  2.01502026\n",
            "  0.51163498 -0.45015471 -1.01227525  0.43437743]\n",
            "  Gradients after quantization:  [ 0.98096273 -2.06731815 -0.64046327 -1.55656896 -0.42967789  2.01867537\n",
            "  0.51074919 -0.45399928 -1.01339125  0.43778502]\n",
            "\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 1.\n",
            "Worker 3, Epoch 2:\n",
            "  Gradients before quantization: [ 0.96976834 -2.04664496 -0.63716678 -1.53707312 -0.42395205  1.99483351\n",
            "  0.50652749 -0.44561472 -1.00214133  0.42999958]\n",
            "  Gradients after quantization:  [ 0.9711531  -2.04664496 -0.63405864 -1.54100327 -0.42538111  1.99848861\n",
            "  0.5056417  -0.44945929 -1.00325734  0.43340717]\n",
            "\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 2.\n",
            "Worker 3, Epoch 3:\n",
            "  Gradients before quantization: [ 0.96005681 -2.02617852 -0.63082619 -1.52166309 -0.41969824  1.97484862\n",
            "  0.50147107 -0.44112012 -0.99210876  0.42566551]\n",
            "  Gradients after quantization:  [ 0.96144157 -2.02617852 -0.62771805 -1.52559323 -0.4211273   1.97850373\n",
            "  0.50058528 -0.44496469 -0.99322476  0.4290731 ]\n",
            "\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 3.\n",
            "Worker 3, Epoch 4:\n",
            "  Gradients before quantization: [ 0.95044239 -2.00591673 -0.62454901 -1.50640715 -0.41548697  1.95506359\n",
            "  0.49646522 -0.43667048 -0.98217651  0.42137478]\n",
            "  Gradients after quantization:  [ 0.95182715 -2.00591673 -0.62144087 -1.5103373  -0.41691603  1.95871869\n",
            "  0.49557943 -0.44051505 -0.98329251  0.42478237]\n",
            "\n",
            "Worker 3 updated model parameters with quantized gradients at epoch 4.\n",
            "\n",
            "Final model parameters: [-0.81168684 -1.44427875 -0.03324162 -0.99812581  0.76440832  0.82237591\n",
            "  0.68385074 -1.44226392 -0.45078338  0.8456939 ]\n"
          ]
        }
      ]
    }
  ]
}