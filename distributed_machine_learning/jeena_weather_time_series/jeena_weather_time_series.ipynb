{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "```\n",
    "Type: Time Series\n",
    "Dataset: jena_climate_2009_2016.csv\n",
    "Model architecture: LSTM and Ensemble of models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==2.8.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.19.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.28.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.17.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (4.25.4)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (1.26.19)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click<9,>=8.0.0->kfp==2.8.0) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (1.63.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp==2.8.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp==2.8.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp==2.8.0) (4.7.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.8.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.8.0) (2.7.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.8.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.8.0) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.8.0) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp==2.8.0) (70.0.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp==2.8.0) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.8.0) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (3.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.8.0) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Readme file to install additional components for kubeflow pipeline such as kserve, push gateway, minio s3 storage alike, cluster roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Walkthrough as follows\n",
    "\n",
    "```\n",
    "python version 3.8.9\n",
    "kfp version 2.8.0\n",
    "kubernetes version 1.29.1\n",
    "kubeflow pipeline version 2.2.0\n",
    "```\n",
    "\n",
    "**Pipeline componentes:**\n",
    "- Data Prepration component     (Data downloading and preprocessing step)\n",
    "- Training Model component     (Model training using both LSTM and ensemble of models Step)\n",
    "  - Training Model using LSTM architecture Component\n",
    "  - Training Model using ensemble of models architecture Component\n",
    "- Evaluating Trained Model component      (Evaluation Step)\n",
    "- Backtesting on Trained Model component    (Backtesting Step for time series data)\n",
    "- Perforamance Check for best model to deploy Component    (Best model finding between trained model step)\n",
    "- Deploy Model Component    (Deploying the Best model)\n",
    "- Monitor Model Component    (Monitor The deployed model)\n",
    "\n",
    "**Arguments for kubefow time-series experiment pipeline as name depicts to be define:**\n",
    "```\n",
    "arguments={\n",
    "        \"epochs\": 1, \n",
    "        \"service_account_name\" : \"sa-minio-kserve\", \n",
    "        \"namespace\" : \"kubeflow\", \n",
    "        \"service_name\" : \"weather-forcast-service\",\n",
    "        \"kserve_version\" :\"v1beta1\"\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2.dsl import component, Input, Output, Dataset, Model, Metrics\n",
    "from kfp import dsl\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepration Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['pandas', 'numpy', 'scikit-learn', 'tensorflow'])\n",
    "def preprocess_data(output_data: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import requests\n",
    "    import zipfile\n",
    "    import io\n",
    "    import os\n",
    "    \n",
    "    url = 'https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for HTTP errors\n",
    "    zip_file = io.BytesIO(response.content)\n",
    "\n",
    "    # Unzip the file\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n",
    "    \n",
    "    csv_path = 'jena_climate_2009_2016.csv'\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Convert the date-time column to datetime format and drop it from the DataFrame\n",
    "    df['Date Time'] = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
    "    df_numeric = df.drop(columns=['Date Time'])  # Keep only numeric columns\n",
    "\n",
    "    # Handle missing values\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df_numeric), columns=df_numeric.columns)\n",
    "\n",
    "    # Handle outliers using RobustScaler\n",
    "    scaler = RobustScaler()\n",
    "    df_scaled = pd.DataFrame(scaler.fit_transform(df_imputed), columns=df_imputed.columns)\n",
    "\n",
    "    # Split the data into train, validation, and test sets\n",
    "    train_val, test = train_test_split(df_scaled, test_size=0.2, shuffle=False)\n",
    "    train, val = train_test_split(train_val, test_size=0.2, shuffle=False)\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    data_path = output_data.path\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    np.save(os.path.join(data_path, 'train.npy'), train)\n",
    "    np.save(os.path.join(data_path, 'val.npy'), val)\n",
    "    np.save(os.path.join(data_path, 'test.npy'), test)\n",
    "    \n",
    "    print(\"Data preprocessing is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model using LSTM architecture Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'scikit-learn'])\n",
    "def train_model(input_data: Input[Dataset], trained_model: Output[Model], train_metrics: Output[Metrics], epochs: int = 10):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    # Load preprocessed data\n",
    "    train_data = np.load(os.path.join(input_data.path, 'train.npy'))\n",
    "    val_data = np.load(os.path.join(input_data.path, 'val.npy'))\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    def create_dataset(data, time_steps=1):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - time_steps):\n",
    "            v = data[i:(i + time_steps), 0]\n",
    "            X.append(v)\n",
    "            y.append(data[i + time_steps, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    time_steps = 24  # Use 24 hours of data to predict the next hour\n",
    "    X_train, y_train = create_dataset(train_data, time_steps)\n",
    "    X_val, y_val = create_dataset(val_data, time_steps)\n",
    "    \n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], 1))\n",
    "    \n",
    "    # Define and compile LSTM model\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(50, activation='relu', input_shape=(time_steps, 1)),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), verbose=1)\n",
    "    \n",
    "    # Save the trained model\n",
    "    model.save(os.path.join(trained_model.path, '1')) ## here '1' is version of model\n",
    "    \n",
    "    # Log metrics\n",
    "    train_metrics.log_metric(\"train_loss\", history.history['loss'][-1])\n",
    "    train_metrics.log_metric(\"val_loss\", history.history['val_loss'][-1])\n",
    "    \n",
    "    print(\"Model training is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model using ensemble of models architecture Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'scikit-learn'])\n",
    "def ensemble_models(input_data: Input[Dataset], input_model: Input[Model], ensemble_model: Output[Model], ensemble_metrics: Output[Metrics]):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "    # Load test data\n",
    "    test_data = np.load(os.path.join(input_data.path, 'test.npy'))\n",
    "    \n",
    "    # Prepare test data for LSTM\n",
    "    def create_dataset(data, time_steps=1):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - time_steps):\n",
    "            v = data[i:(i + time_steps), 0]\n",
    "            X.append(v)\n",
    "            y.append(data[i + time_steps, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    time_steps = 24\n",
    "    X_test, y_test = create_dataset(test_data, time_steps)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Load the trained model\n",
    "    base_model = tf.keras.models.load_model(os.path.join(input_model.path, '1')) ## here '1' is version of model\n",
    "    \n",
    "    # Create an ensemble of models\n",
    "    models = [base_model]\n",
    "\n",
    "    # Add a CNN model\n",
    "    cnn_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=(time_steps, 1)),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Conv1D(128, 3, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(50, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    cnn_model.compile(optimizer='adam', loss='mse')\n",
    "    models.append(cnn_model)\n",
    "\n",
    "    # Add a simple Dense model\n",
    "    dense_model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(time_steps, 1)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    dense_model.compile(optimizer='adam', loss='mse')\n",
    "    models.append(dense_model)\n",
    "    \n",
    "    # Train additional models\n",
    "    for model in models[1:]:\n",
    "        model.fit(X_test, y_test, epochs=5, verbose=0)\n",
    "    \n",
    "    # Make predictions with the ensemble\n",
    "    ensemble_predictions = np.mean([model.predict(X_test) for model in models], axis=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "    mae = mean_absolute_error(y_test, ensemble_predictions)\n",
    "    \n",
    "    # Log metrics\n",
    "    ensemble_metrics.log_metric(\"ensemble_mse\", mse)\n",
    "    ensemble_metrics.log_metric(\"ensemble_mae\", mae)\n",
    "    \n",
    "    # Save the ensemble model\n",
    "    ensemble_model_path = os.path.join(ensemble_model.path, '1') ## here '1' is version of ensemble of models\n",
    "    os.makedirs(ensemble_model_path, exist_ok=True)\n",
    "    for i, model in enumerate(models):\n",
    "        model.save(os.path.join(ensemble_model_path, f'model_{i}'))\n",
    "    \n",
    "    print(\"Ensemble model creation and evaluation is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Trained Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'scikit-learn', 'matplotlib'])\n",
    "def evaluate_model(input_data: Input[Dataset], input_model: Input[Model], evaluation_metrics: Output[Metrics]):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = np.load(os.path.join(input_data.path, 'test.npy'))\n",
    "    \n",
    "    # Prepare test data for LSTM\n",
    "    def create_dataset(data, time_steps=1):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - time_steps):\n",
    "            v = data[i:(i + time_steps), 0]\n",
    "            X.append(v)\n",
    "            y.append(data[i + time_steps, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    time_steps = 24\n",
    "    X_test, y_test = create_dataset(test_data, time_steps)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(os.path.join(input_model.path, '1')) ## here '1' is version of model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    evaluation_metrics.log_metric(\"mse\", mse)\n",
    "    evaluation_metrics.log_metric(\"mae\", mae)\n",
    "    \n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test[:100], label='Actual')\n",
    "    plt.plot(y_pred[:100], label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.savefig(os.path.join(input_model.path, 'actual_vs_predicted.png'))\n",
    "    \n",
    "    print(\"Model evaluation is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting on Trained Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'scikit-learn', 'pandas'])\n",
    "def backtest_model(input_data: Input[Dataset], input_model: Input[Model], backtest_metrics: Output[Metrics]):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = np.load(os.path.join(input_data.path, 'test.npy'))\n",
    "    \n",
    "    # Prepare test data for LSTM\n",
    "    def create_dataset(data, time_steps=1):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - time_steps):\n",
    "            v = data[i:(i + time_steps), 0]\n",
    "            X.append(v)\n",
    "            y.append(data[i + time_steps, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    time_steps = 24  # 24 hours\n",
    "    X_test, y_test = create_dataset(test_data, time_steps)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = tf.keras.models.load_model(os.path.join(input_model.path, '1')) ## here '1' is version of model\n",
    "    \n",
    "    # Initialize arrays for backtesting results\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    # Perform backtesting\n",
    "    # for i in range(X_test.shape[0]):\n",
    "    for i in range(500):\n",
    "        # Make a prediction\n",
    "        X_input = X_test[i:i+1]\n",
    "        y_pred = model.predict(X_input)\n",
    "        \n",
    "        # Store the prediction and actual value\n",
    "        predictions.append(y_pred[0][0])\n",
    "        actuals.append(y_test[i])\n",
    "    \n",
    "    # Convert lists to numpy arrays for easier metric calculation\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    \n",
    "    # Log metrics\n",
    "    backtest_metrics.log_metric(\"backtest_mse\", mse)\n",
    "    backtest_metrics.log_metric(\"backtest_mae\", mae)\n",
    "    \n",
    "    print(f\"Backtesting complete. MSE: {mse}, MAE: {mae}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perforamance Check for best model to deploy Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def preformance_check(\n",
    "    evaluation_metrics: Input[Metrics],\n",
    "    ensemble_metrics: Input[Metrics],\n",
    "    deploy_model_metrics: Output[Metrics],\n",
    "    ensemble_model: Input[Model],\n",
    "    train_model: Input[Model],\n",
    "    deploy_model: Output[Model]):\n",
    "    # Create list of tuples with model metrics and corresponding model\n",
    "    models = [\n",
    "        (evaluation_metrics.metadata.get('mse'), evaluation_metrics.metadata.get('mae')),\n",
    "        (ensemble_metrics.metadata.get('ensemble_mse'), ensemble_metrics.metadata.get('ensemble_mae'))\n",
    "    ]\n",
    "\n",
    "    # Find the index of the model with the minimum MSE and MAE\n",
    "    best_model_index = min(range(len(models)), key=lambda i: (models[i][0], models[i][1]))\n",
    "\n",
    "    # Update deploy_model_metrics and deploy_model based on the selected model\n",
    "    if best_model_index == 0:\n",
    "        deploy_model_metrics.metadata.update(evaluation_metrics.metadata)\n",
    "        deploy_model.path = train_model.path  \n",
    "    else:\n",
    "        deploy_model_metrics.metadata.update(ensemble_metrics.metadata)\n",
    "        deploy_model.path =  ensemble_model.path \n",
    "    \n",
    "    # Output the results\n",
    "    print(\"Deploy Model Metrics:\", deploy_model_metrics.metadata)\n",
    "    print(\"Deploy Model:\", deploy_model.path)\n",
    "    print(\"\\n---- Models Performance Check Complete ----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model using kserve Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['kserve', 'kubernetes'])\n",
    "def deploy_model(input_model: Input[Model], service_account_name:str=\"sa-minio-kserve\", namespace:str= \"kubeflow\", service_name:str=\"weather-model\", kserve_version:str=\"v1beta1\"):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    uri = input_model.uri.replace('minio://', '')\n",
    "    input_model_path = f\"s3://{uri}\"\n",
    "\n",
    "    # namespace = utils.get_default_target_namespace()\n",
    "    \n",
    "\n",
    "    config.load_incluster_config()\n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=service_name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=service_account_name,\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=input_model_path))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "    print(f'Model deployed as an InferenceService: {service_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Deploy Model using prometheus Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['prometheus-client', 'matplotlib', 'psutil'])\n",
    "def monitor_model():\n",
    "    \"\"\"\n",
    "    This component simulates model monitoring by generating and pushing\n",
    "    performance metrics to a Prometheus Pushgateway.\n",
    "    \"\"\"\n",
    "    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "    import time\n",
    "    import psutil  # To monitor resource utilization\n",
    "\n",
    "    # Initialize Prometheus registry and gauges\n",
    "    registry = CollectorRegistry()\n",
    "    performance_gauge = Gauge('model_performance', 'Track model performance', registry=registry)\n",
    "    cpu_usage_gauge = Gauge('cpu_usage', 'Track CPU usage percentage', registry=registry)\n",
    "    memory_usage_gauge = Gauge('memory_usage', 'Track memory usage percentage', registry=registry)\n",
    "\n",
    "    for i in range(10):\n",
    "        # Simulating metrics update\n",
    "        accuracy = 0.85 + 0.01 * i  # Simulate increasing accuracy\n",
    "        performance_gauge.set(accuracy)\n",
    "\n",
    "        # Capture resource utilization\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        cpu_usage_gauge.set(cpu_usage)\n",
    "        memory_usage_gauge.set(memory_usage)\n",
    "\n",
    "        # Push metrics to Prometheus Pushgateway\n",
    "        push_to_gateway('pushgateway:9091', job='model_monitoring', registry=registry)\n",
    "        time.sleep(30)  # Sleep for 30 seconds to simulate real-time tracking\n",
    "\n",
    "    print(\"Model performance and resource utilization metrics sent to Prometheus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined all above Components for Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Time Series Forecasting Pipeline',\n",
    "    description='A pipeline to train, deploy, and monitor a time series forecasting model.'\n",
    ")\n",
    "def time_series_pipeline(epochs: int = 10, service_account_name:str=\"sa-minio-kserve\", namespace:str= \"kubeflow\", service_name:str=\"weather-model\", kserve_version:str=\"v1beta1\"):\n",
    "\n",
    "    preprocess_task = preprocess_data()\n",
    "\n",
    "    train_task = train_model(\n",
    "        input_data=preprocess_task.outputs['output_data'], \n",
    "        epochs=epochs)\n",
    "    \n",
    "    evaluate_task = evaluate_model(\n",
    "        input_data=preprocess_task.outputs['output_data'], \n",
    "        input_model=train_task.outputs['trained_model'])\n",
    "    \n",
    "    ensemble_task = ensemble_models(\n",
    "        input_data=preprocess_task.outputs['output_data'], \n",
    "        input_model=train_task.outputs['trained_model'])\n",
    "    \n",
    "    preformance_check_task = preformance_check(\n",
    "        evaluation_metrics =evaluate_task.outputs['evaluation_metrics'],\n",
    "        ensemble_metrics = ensemble_task.outputs['ensemble_metrics'],\n",
    "        ensemble_model = ensemble_task.outputs['ensemble_model'],\n",
    "        train_model = train_task.outputs['trained_model']\n",
    "    )\n",
    "\n",
    "    deploy_task = deploy_model(\n",
    "        input_model=preformance_check_task.outputs['deploy_model'],\n",
    "        service_account_name=service_account_name, \n",
    "        namespace= namespace, \n",
    "        service_name= service_name,\n",
    "        kserve_version= kserve_version\n",
    "    )\n",
    "    \n",
    "\n",
    "    monitor_task = monitor_model().after(deploy_task)\n",
    "\n",
    "    preprocess_task.set_caching_options(True)\n",
    "    train_task.set_caching_options(True)\n",
    "    evaluate_task.set_caching_options(True)\n",
    "    ensemble_task.set_caching_options(True)\n",
    "    preformance_check_task.set_caching_options(True)\n",
    "    deploy_task.set_caching_options(False)\n",
    "    monitor_task.set_caching_options(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Kubeflow pipline and Run it on Kubefow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/2f91a758-a2fb-4fa2-a948-d1dca66dee79\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/7b7d1378-ffe7-4ee9-9baf-953e0fcdb71f\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=7b7d1378-ffe7-4ee9-9baf-953e0fcdb71f)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=time_series_pipeline,\n",
    "    package_path='time_series_pipeline.yaml'\n",
    ")\n",
    "\n",
    "client = kfp.Client()\n",
    "\n",
    "client.create_run_from_pipeline_func(\n",
    "    time_series_pipeline,\n",
    "    arguments={\n",
    "        \"epochs\": 1, \n",
    "        \"service_account_name\" : \"sa-minio-kserve\", \n",
    "        \"namespace\" : \"kubeflow\", \n",
    "        \"service_name\" : \"weather-forcast-service\",\n",
    "        \"kserve_version\" :\"v1beta1\"\n",
    "        },\n",
    "\n",
    "    experiment_name='time_series_experiment'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
