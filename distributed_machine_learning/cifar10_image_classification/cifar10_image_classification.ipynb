{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Details\n",
    "\n",
    "```\n",
    "Type: Image Classification\n",
    "Dataset: cifar10\n",
    "Model architecture: simple cnn, ResNet50, InceptionV3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kfp==2.8.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.16)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.19.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.28.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.17.0)\n",
      "Requirement already satisfied: kfp-pipeline-spec==0.3.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.3.0)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (26.1.0)\n",
      "Requirement already satisfied: protobuf<5,>=4.21.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (4.25.4)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (1.26.19)\n",
      "Requirement already satisfied: typing-extensions<5,>=3.7.4 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp==2.8.0) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from click<9,>=8.0.0->kfp==2.8.0) (0.4.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (1.63.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp==2.8.0) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp==2.8.0) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.1->kfp==2.8.0) (4.7.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.8.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.8.0) (2.7.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.8.0) (1.5.0)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.8.0) (1.16.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.8.0) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.8.0) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp==2.8.0) (70.0.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp==2.8.0) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from kubernetes<27,>=8.0.0->kfp==2.8.0) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.8.0) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->kfp==2.8.0) (3.6)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\harshav kumar.quantiphi-2611\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.8.0) (3.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kfp==2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Readme file to install additional components for kubeflow pipeline such as kserve, push gateway, minio s3 storage alike, cluster roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Walkthrough as follows\n",
    "\n",
    "```\n",
    "python 3.8.9\n",
    "kfp version 2.8.0\n",
    "kubernetes version 1.29.1\n",
    "kubeflow pipeline version 2.2.0\n",
    "```\n",
    "\n",
    "**Pipeline componentes:**\n",
    "- Data Prepration component     (Data downloading and preprocessing step)\n",
    "- Training Model component     (Model training using simple cnn, ResNet50, and InceptionV3 Step)\n",
    "  - Training Model using simple cnn architecture Component\n",
    "  - Training Model using ResNet50 architecture Component\n",
    "  - Training Model using InceptionV3 architecture Component\n",
    "- Evaluating Trained Model component      (Evaluation Step)\n",
    "- Perforamance Check for best model to deploy Component    (Best model finding between trained model step)\n",
    "- Deploy Model Component    (Deploying the Best model)\n",
    "- Monitor Model Component    (Monitor The deployed model)\n",
    "\n",
    "**Arguments for kubefow time-series experiment pipeline as name depicts to be define:**\n",
    "```\n",
    "arguments={\n",
    "        \"epochs\": 1, \n",
    "        \"service_account_name\" : \"sa-minio-kserve\", \n",
    "        \"namespace\" : \"kubeflow\", \n",
    "        \"service_name\" : \"cifar10-service\",\n",
    "        \"kserve_version\" :\"v1beta1\"\n",
    "        }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harshav Kumar.QUANTIPHI-2611\\AppData\\Local\\Temp\\ipykernel_9828\\3801987161.py:1: FutureWarning: KFP will drop support for Python 3.8 on Oct 1, 2024. To use new versions of the KFP SDK after that date, you will need to upgrade to Python >= 3.9. See https://devguide.python.org/versions/ for more details.\n",
      "  import kfp\n",
      "C:\\Users\\Harshav Kumar.QUANTIPHI-2611\\AppData\\Local\\Temp\\ipykernel_9828\\3801987161.py:2: DeprecationWarning: The module `kfp.v2` is deprecated and will be removed in a futureversion. Please import directly from the `kfp` namespace, instead of `kfp.v2`.\n",
      "  from kfp.v2.dsl import component, Input, Output, Dataset, Model, OutputPath, Metrics, ClassificationMetrics\n"
     ]
    }
   ],
   "source": [
    "import kfp\n",
    "from kfp.v2.dsl import component, Input, Output, Dataset, Model, OutputPath, Metrics, ClassificationMetrics\n",
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prepration Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Harshav Kumar.QUANTIPHI-2611\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\kfp\\dsl\\component_decorator.py:119: FutureWarning: The default base_image used by the @dsl.component decorator will switch from 'python:3.8' to 'python:3.9' on Oct 1, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.9.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    }
   ],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'matplotlib'])\n",
    "def prepare_data(prepare_dataset: Output[Dataset]):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "    data_path = prepare_dataset.path\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "    np.save(os.path.join(data_path, 'x_train.npy'), x_train)\n",
    "    np.save(os.path.join(data_path, 'y_train.npy'), y_train)\n",
    "    np.save(os.path.join(data_path, 'x_test.npy'), x_test)\n",
    "    np.save(os.path.join(data_path, 'y_test.npy'), y_test)\n",
    "\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        plt.imshow(x_train[i])\n",
    "        # The CIFAR labels happen to be arrays, \n",
    "        # which is why you need the extra index\n",
    "        plt.xlabel(class_names[y_train[i][0]])\n",
    "    plt.savefig(data_path + '/dataset.png')\n",
    "\n",
    "    print(\"Data preparation is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model using simple cnn architecture Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'matplotlib', 'scikit-learn'])\n",
    "def train_model_simple_cnn(input_data: Input[Dataset], trained_model_simple_cnn: Output[Model], train_simple_cnn_metrics: Output[Metrics], epochs: int = 10):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train = np.load(os.path.join(input_data.path, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(input_data.path, 'y_train.npy'))\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "            tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(datagen.flow(x_train, y_train, batch_size=64), validation_data=(x_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "    # Save the model\n",
    "    model.save(os.path.join(trained_model_simple_cnn.path, '1'))  ## here '1' is version of model\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(trained_model_simple_cnn.path + '/training_history.png')\n",
    "\n",
    "    train_simple_cnn_metrics.log_metric(\"accuracy\", max(history.history['accuracy']))\n",
    "    train_simple_cnn_metrics.log_metric(\"val_accuracy\", max(history.history['val_accuracy']))\n",
    "\n",
    "    print(\"Model training is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model using ResNet50 architecture Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'matplotlib', 'scikit-learn'])\n",
    "def train_model_resnet50(input_data: Input[Dataset], trained_model_resnet50: Output[Model], train_resnet50_metrics: Output[Metrics], epochs: int = 10):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train = np.load(os.path.join(input_data.path, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(input_data.path, 'y_train.npy'))\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "        base_model = tf.keras.applications.ResNet50(weights=None, include_top=False, input_shape=(32,32,3))\n",
    "\n",
    "        x= base_model.output\n",
    "        x= tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x= tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "        predictions = tf.keras.layers.Dense(10,activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(datagen.flow(x_train, y_train, batch_size=64), validation_data=(x_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "    # Save the model\n",
    "    model.save(os.path.join(trained_model_resnet50.path, '1'))    ## here '1' is version of model\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(trained_model_resnet50.path + '/training_history.png')\n",
    "\n",
    "    train_resnet50_metrics.log_metric(\"accuracy\", max(history.history['accuracy']))\n",
    "    train_resnet50_metrics.log_metric(\"val_accuracy\", max(history.history['val_accuracy']))\n",
    "\n",
    "    print(\"Model training is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model using InceptionV3 Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'matplotlib', 'scikit-learn'])\n",
    "def train_model_inceptionv3(input_data: Input[Dataset], trained_model_inceptionv3: Output[Model], train_inceptionv3_metrics: Output[Metrics], epochs: int = 10):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    x_train = np.load(os.path.join(input_data.path, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(input_data.path, 'y_train.npy'))\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        zoom_range=0.1\n",
    "    )\n",
    "\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "    print(f'Number of devices:{strategy.num_replicas_in_sync}')\n",
    "\n",
    "    with strategy.scope():\n",
    "\n",
    "        inputs = tf.keras.layers.Input(shape=(32,32,3))\n",
    "        resized_input = tf.keras.layers.Resizing(75,75)(inputs)\n",
    "\n",
    "        base_model = tf.keras.applications.InceptionV3(weights=None, include_top=False, input_tensor=resized_input)\n",
    "\n",
    "        x= base_model.output\n",
    "        x= tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x= tf.keras.layers.Dense(64,activation='relu')(x)\n",
    "        predictions = tf.keras.layers.Dense(10,activation='softmax')(x)\n",
    "        \n",
    "        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        history = model.fit(datagen.flow(x_train, y_train, batch_size=64), validation_data=(x_val, y_val), epochs=epochs, callbacks=[early_stopping])\n",
    "\n",
    "    # Save the model\n",
    "    model.save(os.path.join(trained_model_inceptionv3.path, '1'))   ## here '1' is version of model\n",
    "\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim([0, 1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.savefig(trained_model_inceptionv3.path + '/training_history.png')\n",
    "\n",
    "    train_inceptionv3_metrics.log_metric(\"accuracy\", max(history.history['accuracy']))\n",
    "    train_inceptionv3_metrics.log_metric(\"val_accuracy\", max(history.history['val_accuracy']))\n",
    "\n",
    "    print(\"Model training is complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Trained Model Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['tensorflow', 'numpy', 'scikit-learn', 'seaborn', 'matplotlib'])\n",
    "def evaluate_model(input_data: Input[Dataset], input_model: Input[Model], classification_report_metrics: Output[ClassificationMetrics], evalution_metrics: Output[Metrics]):\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import seaborn as sns\n",
    "    import json\n",
    "\n",
    "    x_test = np.load(os.path.join(input_data.path, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(input_data.path, 'y_test.npy'))\n",
    "\n",
    "    # Normalize pixel values to be between 0 and 1\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "    model = tf.keras.models.load_model(os.path.join(input_model.path, '1'))  ## here '1' is version of model\n",
    "\n",
    "    # loss, accuracy = model.evaluate(x_test, y_test)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = y_test.flatten()\n",
    "\n",
    "    class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "    report = classification_report(y_true_classes, y_pred_classes, output_dict=True)\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(input_model.path + '/evaluation_metrics_and_confusion_matrix_graph.png')\n",
    "    \n",
    "    # Log the confusion matrix with both matrix and categories arguments\n",
    "    classification_report_metrics.log_confusion_matrix(matrix=cm.tolist(), categories=class_names)\n",
    "\n",
    "    evalution_metrics.log_metric(\"precision\", report['weighted avg']['precision'])\n",
    "    evalution_metrics.log_metric(\"recall\", report['weighted avg']['recall'])\n",
    "    evalution_metrics.log_metric(\"f1-score\", report['weighted avg']['f1-score'])\n",
    "\n",
    "\n",
    "    # print(f'Model evaluation complete. Loss: {loss}, Accuracy: {accuracy}')\n",
    "    print(\"\\n---- Model Evaluation Complete ----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perforamance Check for best model to deploy Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component()\n",
    "def preformance_check(evaluation_simple_cnn_metrics: Input[Metrics], \n",
    "                      evaluation_resnet50_metrics: Input[Metrics], \n",
    "                      evaluation_inceptionv3_metrics: Input[Metrics], \n",
    "                      deploy_model_metrics: Output[Metrics], \n",
    "                      trained_model_simple_cnn: Input[Model], \n",
    "                      trained_model_resnet50: Input[Model], \n",
    "                      trained_model_inceptionv3: Input[Model], \n",
    "                      deploy_model: Output[Model]):\n",
    "\n",
    "\n",
    "    # Create list of tuples with model metrics and corresponding model\n",
    "    models = [\n",
    "        (evaluation_simple_cnn_metrics.metadata.get('f1-score'), trained_model_simple_cnn),\n",
    "        (evaluation_resnet50_metrics.metadata.get('f1-score'), trained_model_resnet50),\n",
    "        (evaluation_inceptionv3_metrics.metadata.get('f1-score'), trained_model_inceptionv3)\n",
    "    ]\n",
    "\n",
    "    # Find model with maximum F1 score\n",
    "    max_f1_score, deploy_model = max(models, key=lambda x: x[0])\n",
    "\n",
    "    # Get tmetrics corresponding to selected model\n",
    "    if max_f1_score == evaluation_simple_cnn_metrics.metadata.get('f1-score'):\n",
    "        deploy_model_metrics.metadata.update(evaluation_simple_cnn_metrics.metadata)\n",
    "        deploy_model.path = trained_model_simple_cnn.path\n",
    "        \n",
    "    elif max_f1_score == evaluation_resnet50_metrics.metadata.get('f1-score'):\n",
    "        deploy_model_metrics.metadata.update(evaluation_resnet50_metrics.metadata)\n",
    "        deploy_model.path = trained_model_resnet50.path\n",
    "\n",
    "    else:\n",
    "        deploy_model_metrics.metadata.update(evaluation_inceptionv3_metrics.metadata)\n",
    "        deploy_model.path = trained_model_inceptionv3.path\n",
    "\n",
    "    # Output the results\n",
    "    print(\"Deploy Model Metrics:\", deploy_model_metrics.metadata)\n",
    "    print(\"Deploy Model:\", deploy_model.path)\n",
    "\n",
    "    print(\"\\n---- Models Preformance Check Complete ----\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model using kserve Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['kserve', 'kubernetes'])\n",
    "def deploy_model(input_model: Input[Model], service_account_name:str=\"sa-minio-kserve\", namespace:str= \"kubeflow\", service_name:str=\"weather-model\", kserve_version:str=\"v1beta1\"):\n",
    "    \"\"\"\n",
    "    Create kserve instance\n",
    "    \"\"\"\n",
    "    from kubernetes import client, config\n",
    "    from kserve import KServeClient\n",
    "    from kserve import constants\n",
    "    from kserve import utils\n",
    "    from kserve import V1beta1InferenceService\n",
    "    from kserve import V1beta1InferenceServiceSpec\n",
    "    from kserve import V1beta1PredictorSpec\n",
    "    from kserve import V1beta1TFServingSpec\n",
    "    from datetime import datetime\n",
    "\n",
    "    uri = input_model.uri.replace('minio://', '')\n",
    "    input_model_path = f\"s3://{uri}\"\n",
    "\n",
    "    # namespace = utils.get_default_target_namespace()\n",
    "    \n",
    "\n",
    "    config.load_incluster_config()\n",
    "\n",
    "    now = datetime.now()\n",
    "    v = now.strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "\n",
    "    api_version = constants.KSERVE_GROUP + '/' + kserve_version\n",
    "\n",
    "    isvc = V1beta1InferenceService(api_version=api_version,\n",
    "                                   kind=constants.KSERVE_KIND,\n",
    "                                   metadata=client.V1ObjectMeta(\n",
    "                                       name=service_name, namespace=namespace, annotations={'sidecar.istio.io/inject':'false'}),\n",
    "                                   spec=V1beta1InferenceServiceSpec(\n",
    "                                   predictor=V1beta1PredictorSpec(\n",
    "                                       service_account_name=service_account_name,\n",
    "                                       tensorflow=(V1beta1TFServingSpec(\n",
    "                                           storage_uri=input_model_path))))\n",
    "    )\n",
    "\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(isvc)\n",
    "\n",
    "    print(f'Model deployed as an InferenceService: {service_name}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor Deploy Model using prometheus Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(packages_to_install=['prometheus-client', 'matplotlib', 'psutil'])\n",
    "def monitor_model():\n",
    "    \"\"\"\n",
    "    This component simulates model monitoring by generating and pushing\n",
    "    performance metrics to a Prometheus Pushgateway.\n",
    "    \"\"\"\n",
    "    from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "    import time\n",
    "    import psutil  # To monitor resource utilization\n",
    "\n",
    "    # Initialize Prometheus registry and gauges\n",
    "    registry = CollectorRegistry()\n",
    "    performance_gauge = Gauge('model_performance', 'Track model performance', registry=registry)\n",
    "    cpu_usage_gauge = Gauge('cpu_usage', 'Track CPU usage percentage', registry=registry)\n",
    "    memory_usage_gauge = Gauge('memory_usage', 'Track memory usage percentage', registry=registry)\n",
    "\n",
    "    for i in range(10):\n",
    "        # Simulating metrics update\n",
    "        accuracy = 0.85 + 0.01 * i  # Simulate increasing accuracy\n",
    "        performance_gauge.set(accuracy)\n",
    "\n",
    "        # Capture resource utilization\n",
    "        cpu_usage = psutil.cpu_percent()\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        cpu_usage_gauge.set(cpu_usage)\n",
    "        memory_usage_gauge.set(memory_usage)\n",
    "\n",
    "        # Push metrics to Prometheus Pushgateway\n",
    "        push_to_gateway('pushgateway:9091', job='model_monitoring', registry=registry)\n",
    "        time.sleep(30)  # Sleep for 30 seconds to simulate real-time tracking\n",
    "\n",
    "    print(\"Model performance and resource utilization metrics sent to Prometheus.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined all above Components for Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name='Image Classification Pipeline with Serving and Monitoring',\n",
    "    description='An example pipeline to train, deploy, and monitor an image classifier on the CIFAR-10 dataset.'\n",
    ")\n",
    "def image_classification_pipeline(epochs: int = 10, service_account_name:str=\"sa-minio-kserve\", namespace:str= \"kubeflow\", service_name:str=\"weather-model\", kserve_version:str=\"v1beta1\"):\n",
    "    # Define pipeline tasks\n",
    "    preprocess_data_task = prepare_data()\n",
    "\n",
    "    train_simple_cnn_task = train_model_simple_cnn(\n",
    "        input_data=preprocess_data_task.outputs['prepare_dataset'],\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    train_resnet50_task = train_model_resnet50(\n",
    "        input_data=preprocess_data_task.outputs['prepare_dataset'],\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    train_inceptionv3_task = train_model_inceptionv3(\n",
    "        input_data=preprocess_data_task.outputs['prepare_dataset'],\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    evaluate_model_task_simple_cnn = evaluate_model(\n",
    "        input_data=preprocess_data_task.outputs['prepare_dataset'],\n",
    "        input_model=train_simple_cnn_task.outputs['trained_model_simple_cnn']\n",
    "    )\n",
    "\n",
    "    evaluate_model_task_resnet50 = evaluate_model(\n",
    "        input_data=preprocess_data_task.outputs['prepare_dataset'],\n",
    "        input_model=train_resnet50_task.outputs['trained_model_resnet50']\n",
    "    )\n",
    "\n",
    "    evaluate_model_task_inceptionv3 = evaluate_model(\n",
    "        input_data=preprocess_data_task.outputs['prepare_dataset'],\n",
    "        input_model=train_inceptionv3_task.outputs['trained_model_inceptionv3']\n",
    "    )\n",
    "\n",
    "    preformance_check_task = preformance_check(\n",
    "        evaluation_simple_cnn_metrics = evaluate_model_task_simple_cnn.outputs['evalution_metrics'],\n",
    "        evaluation_resnet50_metrics= evaluate_model_task_resnet50.outputs['evalution_metrics'],\n",
    "        evaluation_inceptionv3_metrics= evaluate_model_task_inceptionv3.outputs['evalution_metrics'],\n",
    "        trained_model_simple_cnn = train_simple_cnn_task.outputs['trained_model_simple_cnn'],\n",
    "        trained_model_resnet50 = train_resnet50_task.outputs['trained_model_resnet50'],\n",
    "        trained_model_inceptionv3= train_inceptionv3_task.outputs['trained_model_inceptionv3']\n",
    "    )\n",
    "\n",
    "    deploy_model_task = deploy_model(\n",
    "        input_model=preformance_check_task.outputs['deploy_model'],\n",
    "        service_account_name=service_account_name, \n",
    "        namespace= namespace, \n",
    "        service_name= service_name,\n",
    "        kserve_version= kserve_version\n",
    "    )\n",
    "\n",
    "    monitor_pipeline_task = monitor_model().after(deploy_model_task)\n",
    "\n",
    "    # Enable caching for all tasks\n",
    "    preprocess_data_task.set_caching_options(True)\n",
    "    train_simple_cnn_task.set_caching_options(True)\n",
    "    train_resnet50_task.set_caching_options(True)\n",
    "    train_inceptionv3_task.set_caching_options(True)\n",
    "    evaluate_model_task_simple_cnn.set_caching_options(True)\n",
    "    evaluate_model_task_resnet50.set_caching_options(True)\n",
    "    evaluate_model_task_inceptionv3.set_caching_options(True)\n",
    "    preformance_check_task.set_caching_options(True)\n",
    "    deploy_model_task.set_caching_options(False)\n",
    "    monitor_pipeline_task.set_caching_options(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Kubeflow pipline and Run it on Kubefow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/8ad7ed0e-b1b5-40f3-899e-fab4288382d8\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/db2f2799-ca05-4c3b-b67a-98bf82539f22\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=db2f2799-ca05-4c3b-b67a-98bf82539f22)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Compile the pipeline\n",
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=image_classification_pipeline,\n",
    "    package_path='image_classification.yaml'\n",
    ")\n",
    "\n",
    "# Create a client to run the pipeline\n",
    "client = kfp.Client()\n",
    "\n",
    "# Run the pipeline\n",
    "client.create_run_from_pipeline_func(\n",
    "    image_classification_pipeline,\n",
    "    arguments={\n",
    "        \"epochs\": 1, \n",
    "        \"service_account_name\" : \"sa-minio-kserve\", \n",
    "        \"namespace\" : \"kubeflow\", \n",
    "        \"service_name\" : \"cifar10-service\",\n",
    "        \"kserve_version\" :\"v1beta1\"\n",
    "        },\n",
    "    experiment_name='image_classification_experiment'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
