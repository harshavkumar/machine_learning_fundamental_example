{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Distribute the dataset across multiple workers.\n",
        "*   Perform k-means clustering on each subset of the data.\n",
        "*   Aggregate the results to form a global clustering solution.\n",
        "\n"
      ],
      "metadata": {
        "id": "vU0xEa2IRbtl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAhJosNKRVee",
        "outputId": "47810820-14f9-47ec-9528-46cf439dc134"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=dbef1097a39293474ece1fa218e8d1ed7213e486297d6a5c4a6d5a6a00bc3cd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DistributedKMeans\").getOrCreate()\n",
        "\n",
        "# Sample data (use your dataset here)\n",
        "data = pd.DataFrame({\n",
        "    'x': np.random.rand(1000),\n",
        "    'y': np.random.rand(1000)\n",
        "})\n",
        "\n",
        "# Convert to Spark DataFrame\n",
        "sdf = spark.createDataFrame(data)\n",
        "\n",
        "# Assemble features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"x\", \"y\"], outputCol=\"features\")\n",
        "dataset = assembler.transform(sdf).select(\"features\")\n",
        "\n",
        "# Define k-means parameters\n",
        "k = 3\n",
        "max_iter = 10\n",
        "\n",
        "# Define the number of partitions (workers)\n",
        "num_partitions = 4\n",
        "\n",
        "# Repartition the dataset\n",
        "partitioned_data = dataset.repartition(num_partitions)\n",
        "\n",
        "# Function to perform k-means on each partition\n",
        "def kmeans_partition(partition, k, max_iter):\n",
        "    # Remove SparkSession creation here - use the existing 'spark' from the global scope\n",
        "    partition_df = spark.createDataFrame(partition, schema=[\"features\"]) # Use the existing 'spark' session\n",
        "    kmeans = KMeans(k=k, maxIter=max_iter)\n",
        "    model = kmeans.fit(partition_df)\n",
        "    centers = model.clusterCenters()\n",
        "    return centers\n",
        "\n",
        "# Apply k-means to each partition\n",
        "partition_centers = partitioned_data.rdd.mapPartitions(\n",
        "    lambda partition: kmeans_partition(partition, k, max_iter)\n",
        ").collect()\n",
        "\n",
        "# Aggregate centers from all partitions\n",
        "global_centers = np.array(partition_centers)\n",
        "\n",
        "# Perform final k-means clustering on the aggregated centers\n",
        "final_kmeans = KMeans(k=k, maxIter=max_iter)\n",
        "final_model = final_kmeans.fit(spark.createDataFrame(pd.DataFrame(global_centers, columns=[\"x\", \"y\"])))\n",
        "\n",
        "# Print final cluster centers\n",
        "print(\"Final Cluster Centers: \")\n",
        "for center in final_model.clusterCenters():\n",
        "    print(center)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 708
        },
        "id": "_8-gHqfrRY5X",
        "outputId": "dfcd3f46-29ab-4b53-a6ab-4c1649cceb7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 459, in dumps\n",
            "    return cloudpickle.dumps(obj, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
            "    cp.dump(obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\", line 632, in dump\n",
            "    return Pickler.dump(self, obj)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 466, in __getnewargs__\n",
            "    raise PySparkRuntimeError(\n",
            "pyspark.errors.exceptions.base.PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickleError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     72\u001b[0m             )\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__getnewargs__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;31m# This method is called when attempting to pickle SparkContext, which is always an error:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         raise PySparkRuntimeError(\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0merror_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CONTEXT_ONLY_VALID_ON_DRIVER\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkRuntimeError\u001b[0m: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-2b30439e3b95>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m partition_centers = partitioned_data.rdd.mapPartitions(\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mlambda\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkmeans_partition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m ).collect()\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m# Aggregate centers from all partitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1833\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1834\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_jrdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5468\u001b[0m             \u001b[0mprofiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5470\u001b[0;31m         wrapped_func = _wrap_function(\n\u001b[0m\u001b[1;32m   5471\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd_deserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5472\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[1;32m   5266\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"serializer should not be empty\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5267\u001b[0m     \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofiler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5268\u001b[0;31m     \u001b[0mpickled_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbroadcast_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincludes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_for_python_RDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5269\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5270\u001b[0m     return sc._jvm.SimplePythonFunction(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36m_prepare_for_python_RDD\u001b[0;34m(sc, command)\u001b[0m\n\u001b[1;32m   5249\u001b[0m     \u001b[0;31m# the serialized command will be compressed by broadcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[0mser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5251\u001b[0;31m     \u001b[0mpickled_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5252\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5253\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickled_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBroadcastThreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Default 1M\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Could not serialize object: %s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mprint_exec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Could not serialize object: PySparkRuntimeError: [CONTEXT_ONLY_VALID_ON_DRIVER] It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from sklearn.cluster import KMeans, SpectralClustering\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Node class representing each node in the distributed system\n",
        "class Node:\n",
        "    def __init__(self, neighbors, degree):\n",
        "        self.neighbors = neighbors\n",
        "        self.degree = degree\n",
        "        self.data = None\n",
        "        self.centers = None\n",
        "        self.local_coreset = None\n",
        "        self.weights = None\n",
        "        self.message_received = {}\n",
        "        self.X = None\n",
        "        self.cost_of_each_data = None\n",
        "\n",
        "    def set_data(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def set_centers(self, centers):\n",
        "        self.centers = centers\n",
        "\n",
        "    def set_cost_of_each_data(self, c):\n",
        "        self.cost_of_each_data = c\n",
        "\n",
        "    def set_local_coreset(self, S):\n",
        "        self.local_coreset = S\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        self.weights = weights\n",
        "\n",
        "    def set_X(self, X):\n",
        "        self.X = X\n",
        "\n",
        "# Initialize communication and cost tracking\n",
        "c_cost = np.zeros(8)\n",
        "k_means_cost = np.zeros(8)\n",
        "communication_cost = 0\n",
        "\n",
        "# Functions to create different types of graph topologies\n",
        "def create_random_graph(no_of_nodes, probability):\n",
        "    G = nx.erdos_renyi_graph(no_of_nodes, probability)\n",
        "    while not nx.is_connected(G):\n",
        "        G = create_random_graph(no_of_nodes, probability)\n",
        "    return G\n",
        "\n",
        "def create_preferential_graph(n, m):\n",
        "    G = nx.barabasi_albert_graph(n, m)\n",
        "    while not nx.is_connected(G):\n",
        "        G = create_preferential_graph(n, m)\n",
        "    return G\n",
        "\n",
        "def create_grid_graph(n, m):\n",
        "    return nx.grid_2d_graph(n, m)\n",
        "\n",
        "# Generate a sequence for node traversal based on DFS\n",
        "def node_sequence(G):\n",
        "    seq = []\n",
        "    l = list(nx.dfs_edges(G, 0))\n",
        "    for i in range(len(l) - 1):\n",
        "        if l[i][1] == l[i+1][0]:\n",
        "            seq.append(l[i][0])\n",
        "        else:\n",
        "            seq.append(l[i][0])\n",
        "            seq.append(l[i][1])\n",
        "            p = nx.shortest_path(G, l[i][1], l[i+1][0])\n",
        "            for k in range(1, len(p) - 1):\n",
        "                seq.append(p[k])\n",
        "    seq.append(l[-1][0])\n",
        "    seq.append(l[-1][1])\n",
        "    return seq\n",
        "\n",
        "# Methods for partitioning data across nodes\n",
        "def uniform_partitioning(df, nodes):\n",
        "    temp_df = df.copy(deep=True)\n",
        "    size_of_pi = math.floor(df.shape[0] / len(nodes))\n",
        "    for node in nodes:\n",
        "        if node != nodes[-1]:\n",
        "            node_dict[node].data = temp_df.sample(size_of_pi)\n",
        "            temp_df.drop(node_dict[node].data.index, inplace=True)\n",
        "        else:\n",
        "            node_dict[node].data = temp_df\n",
        "\n",
        "def similarity_partitioning(df, nodes):\n",
        "    temp_df = df.copy(deep=True)\n",
        "    spec = SpectralClustering(n_clusters=len(nodes), gamma=1.0)\n",
        "    c_id = spec.fit_predict(temp_df)\n",
        "    for i in range(len(nodes)):\n",
        "        node_dict[nodes[i]].data = temp_df[c_id == i]\n",
        "\n",
        "def weighted_partitioning(df, nodes):\n",
        "    temp_df = df.copy(deep=True)\n",
        "    s = np.random.normal(0, 1, len(nodes))\n",
        "    s = (abs(s) / np.sum(abs(s))) * temp_df.shape[0]\n",
        "    for i in range(len(nodes)):\n",
        "        if nodes[i] != nodes[-1]:\n",
        "            node_dict[nodes[i]].data = temp_df.sample(int(round(s[i], 0)))\n",
        "            temp_df.drop(node_dict[nodes[i]].data.index, inplace=True)\n",
        "        else:\n",
        "            node_dict[nodes[i]].data = temp_df\n",
        "\n",
        "def degree_partitioning(df, nodes):\n",
        "    temp_df = df.copy(deep=True)\n",
        "    s = []\n",
        "    for node in nodes:\n",
        "        s.append(float(G.degree(node)))\n",
        "    s = (s / np.sum(s)) * temp_df.shape[0]\n",
        "    for i in range(len(nodes)):\n",
        "        if nodes[i] != nodes[-1]:\n",
        "            node_dict[nodes[i]].data = temp_df.sample(round(s[i], 0))\n",
        "            temp_df.drop(node_dict[nodes[i]].data.index, inplace=True)\n",
        "        else:\n",
        "            node_dict[nodes[i]].data = temp_df\n",
        "\n",
        "# K-Means clustering algorithm\n",
        "def clustering_algo(data, no_of_centers):\n",
        "    # Ensure data is 2D\n",
        "    if data.ndim == 1:\n",
        "        data = data.reshape(-1, 1)\n",
        "\n",
        "    # Check if the number of samples is at least the number of clusters\n",
        "    if data.shape[0] < no_of_centers:\n",
        "        print(f\"Warning: Reducing number of clusters to {data.shape[0]} due to insufficient samples.\")\n",
        "        no_of_centers = data.shape[0]\n",
        "\n",
        "    kmeans = KMeans(n_clusters=no_of_centers, init='random', random_state=0).fit(data)\n",
        "    return kmeans\n",
        "\n",
        "# Message Passing algorithm to communicate data between nodes\n",
        "def Message_Passing(message, neighbors, node):\n",
        "    global communication_cost\n",
        "    if node not in node_dict[node].message_received:\n",
        "        node_dict[node].message_received[node] = message\n",
        "    for neighbor in neighbors:\n",
        "        for i, message in node_dict[node].message_received.items():\n",
        "            if i not in node_dict[neighbor].message_received:\n",
        "                if isinstance(message, pd.DataFrame) and message.ndim == 1:\n",
        "                    message = message.to_frame().T  # Convert to 2D by transposing\n",
        "\n",
        "                node_dict[neighbor].message_received[i] = message\n",
        "\n",
        "                # Calculate communication cost based on message type and dimensions\n",
        "                if np.isscalar(message):\n",
        "                    communication_cost += 1\n",
        "                elif len(np.shape(message)) == 1:\n",
        "                    communication_cost += message.shape[0]\n",
        "                elif len(np.shape(message)) == 2:\n",
        "                    communication_cost += message.shape[0] * message.shape[1]\n",
        "\n",
        "\n",
        "# Function to calculate the cost of clustering\n",
        "def get_cost(data, centers):\n",
        "    distanceMatrix = euclidean_distances(data, centers)\n",
        "    return pd.DataFrame(distanceMatrix).min(axis=1)\n",
        "\n",
        "# Distributed coreset construction algorithm\n",
        "def distributed_coreset_construction(nodes, t, no_of_centers):\n",
        "    for node in list(set(nodes)):\n",
        "        node_dict[node].centers = pd.DataFrame(clustering_algo(node_dict[node].data, no_of_centers).cluster_centers_)\n",
        "        node_dict[node].centers.columns = node_dict[node].data.columns\n",
        "        node_dict[node].cost_of_each_data = get_cost(node_dict[node].data, node_dict[node].centers)\n",
        "        node_dict[node].cost_of_each_data.index = node_dict[node].data.index\n",
        "\n",
        "    for node in nodes:\n",
        "        Message_Passing(node_dict[node].cost_of_each_data.sum(), node_dict[node].neighbors, node)\n",
        "\n",
        "    for node in list(reversed(nodes)):\n",
        "        Message_Passing(node_dict[node].cost_of_each_data.sum(), node_dict[node].neighbors, node)\n",
        "\n",
        "    for node in list(set(nodes)):\n",
        "        t_i = int(math.floor((t * node_dict[node].message_received[node]) / sum(node_dict[node].message_received.values())))\n",
        "        m_p = 2 * (node_dict[node].cost_of_each_data + 1e-31)\n",
        "        m_p.index = node_dict[node].data.index\n",
        "        S_i = node_dict[node].data.sample(n=t_i, weights=m_p)\n",
        "        w_q = sum(node_dict[node].message_received.values()) / (t * m_p[S_i.index])\n",
        "        w_b = []\n",
        "        for index, b in node_dict[node].centers.iterrows():\n",
        "            temp_cost = get_cost(node_dict[node].data, b)\n",
        "            temp_cost.index = node_dict[node].data.index\n",
        "            Pb = node_dict[node].data[temp_cost == node_dict[node].cost_of_each_data]\n",
        "            w_b.append(Pb.shape[0] - sum(w_q[S_i.index.intersection(Pb.index)]))\n",
        "        node_dict[node].message_received = {}\n",
        "        node_dict[node].set_local_coreset(pd.concat([S_i, node_dict[node].centers]))\n",
        "        node_dict[node].set_weights(w_q.append(pd.Series(w_b)))\n",
        "\n",
        "# Distributed clustering on graph algorithm\n",
        "def distributed_clustering_on_graph(nodes, t, no_of_centers):\n",
        "    distributed_coreset_construction(nodes, t, no_of_centers)\n",
        "    for v_i in nodes:\n",
        "        Message_Passing(node_dict[v_i].local_coreset, node_dict[v_i].neighbors, v_i)\n",
        "    for v_i in list(reversed(nodes)):\n",
        "        Message_Passing(node_dict[v_i].local_coreset, node_dict[v_i].neighbors, v_i)\n",
        "\n",
        "    combined_data = pd.concat(list(node_dict[v_i].message_received.values()))\n",
        "\n",
        "    # Ensure combined_data is 2D\n",
        "    if combined_data.ndim == 1:\n",
        "        combined_data = combined_data.to_frame().T  # Transpose to ensure it's in correct format\n",
        "\n",
        "    # Avoid clustering if there are fewer samples than clusters\n",
        "    if combined_data.shape[0] < no_of_centers:\n",
        "        print(f\"Skipping clustering for {combined_data.shape[0]} samples with {no_of_centers} clusters.\")\n",
        "        return None\n",
        "\n",
        "    cluster_details = clustering_algo(combined_data, no_of_centers)\n",
        "    return cluster_details\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "\n",
        "n = 4\n",
        "m = 3\n",
        "G = create_preferential_graph(n, m)\n",
        "nodes = node_sequence(G)\n",
        "node_dict = {j: Node(list(G.neighbors(j)), G.degree(j)) for j in G.nodes()}\n",
        "\n",
        "# Load the dataset directly from the UCI repository\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
        "df = pd.read_csv(url, header=None)\n",
        "df = df.fillna(0)\n",
        "df = df.iloc[:, 1:]\n",
        "\n",
        "weighted_partitioning(df, list(G.nodes()))\n",
        "\n",
        "t = np.arange(0.1, 0.8, 0.1) * df.shape[0]\n",
        "no_of_centers = 2\n",
        "\n",
        "for i in range(len(t)):\n",
        "    for node in G.nodes():\n",
        "        node_dict[node].centers = None\n",
        "        node_dict[node].local_coreset = None\n",
        "        node_dict[node].weights = None\n",
        "        node_dict[node].message_received = {}\n",
        "        node_dict[node].X = None\n",
        "        node_dict[node].cost_of_each_data = None\n",
        "\n",
        "    cluster_details = distributed_clustering_on_graph(nodes, t[i], no_of_centers)\n",
        "\n",
        "    if cluster_details is not None:\n",
        "        c_cost[i] = communication_cost\n",
        "        centralised_cluster = KMeans(n_clusters=no_of_centers, init='random', random_state=0).fit(np.array(df))\n",
        "        k_means_cost[i] = pd.DataFrame(euclidean_distances(df, cluster_details.cluster_centers_)).min(axis=1).sum() / pd.DataFrame(euclidean_distances(df, centralised_cluster.cluster_centers_)).min(axis=1).sum()\n",
        "\n",
        "c_cost = c_cost[:-1]\n",
        "k_means_cost = k_means_cost[:-1]\n",
        "\n",
        "fig11 = plt.figure()\n",
        "plt.xlabel('Communication Cost (*10^6)')\n",
        "plt.plot(np.arange(len(c_cost)), list(k_means_cost), '*-b')\n",
        "plt.xticks(np.arange(len(c_cost)), list(np.round(c_cost / 10**6, 2)))\n",
        "plt.ylabel('K Means cost')\n",
        "plt.show()\n",
        "fig11.savefig('spam_random_degree1.png')\n",
        "\n",
        "print('Communication cost is', communication_cost)\n",
        "print('Euclidean distance is', pd.DataFrame(euclidean_distances(df, cluster_details.cluster_centers_)).min(axis=1).sum() / pd.DataFrame(euclidean_distances(df, centralised_cluster.cluster_centers_)).min(axis=1).sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "B9sImmINUHGR",
        "outputId": "433b49f4-3198-4082-a351-feb6c5954d8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected 2D array, got 1D array instead:\narray=[ 1.61304348e-01  4.76956522e-01  5.86956522e-03  4.00434783e-01\n  2.18043478e-01  9.95652174e-02  1.66304348e-01  3.50217391e-01\n  3.47391304e-01  1.94347826e-01  5.81739130e-01  2.33043478e-01\n  2.65434783e-01  3.80000000e-01  3.13913043e-01  2.00000000e-01\n  3.72173913e-01  1.84565217e+00  4.24130435e-01  9.31521739e-01\n  3.10652174e-01  3.72173913e-01  2.11521739e-01  4.39130435e-02\n  1.26086957e-02  8.47826087e-03  1.08695652e-03  6.73913043e-03\n  3.91304348e-03 -1.38777878e-17 -3.46944695e-17  1.67608696e-01\n -2.77555756e-17  2.82608696e-03  6.45652174e-02  1.06304348e-01\n  1.52173913e-03  1.34782609e-02  4.50000000e-02  1.26086957e-02\n  3.91304348e-03  3.56521739e-02  2.28260870e-02  4.39130435e-02\n  6.82608696e-02  2.60869565e-03  2.21739130e-02  2.00652174e-02\n  9.80217391e-02  3.62608696e-02  3.10695652e-01  1.86760870e-01\n  8.23260870e-02  3.43565870e+01  3.78304348e+02  2.24915217e+03\n  7.82608696e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-e782287289a3>\u001b[0m in \u001b[0;36m<cell line: 236>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_of_each_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mcluster_details\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistributed_clustering_on_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcluster_details\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e782287289a3>\u001b[0m in \u001b[0;36mdistributed_clustering_on_graph\u001b[0;34m(nodes, t, no_of_centers)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m# Distributed clustering on graph algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdistributed_clustering_on_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mdistributed_coreset_construction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_of_centers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mv_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mMessage_Passing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_coreset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e782287289a3>\u001b[0m in \u001b[0;36mdistributed_coreset_construction\u001b[0;34m(nodes, t, no_of_centers)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mw_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mtemp_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m             \u001b[0mtemp_cost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mPb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_cost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnode_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcost_of_each_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-e782287289a3>\u001b[0m in \u001b[0;36mget_cost\u001b[0;34m(data, centers)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;31m# Function to calculate the cost of clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mdistanceMatrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistanceMatrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    308\u001b[0m            [1.41421356]])\n\u001b[1;32m    309\u001b[0m     \"\"\"\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_norm_squared\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         )\n\u001b[0;32m--> 173\u001b[0;31m         Y = check_array(\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;31m# If input is 1D raise error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    939\u001b[0m                     \u001b[0;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 1.61304348e-01  4.76956522e-01  5.86956522e-03  4.00434783e-01\n  2.18043478e-01  9.95652174e-02  1.66304348e-01  3.50217391e-01\n  3.47391304e-01  1.94347826e-01  5.81739130e-01  2.33043478e-01\n  2.65434783e-01  3.80000000e-01  3.13913043e-01  2.00000000e-01\n  3.72173913e-01  1.84565217e+00  4.24130435e-01  9.31521739e-01\n  3.10652174e-01  3.72173913e-01  2.11521739e-01  4.39130435e-02\n  1.26086957e-02  8.47826087e-03  1.08695652e-03  6.73913043e-03\n  3.91304348e-03 -1.38777878e-17 -3.46944695e-17  1.67608696e-01\n -2.77555756e-17  2.82608696e-03  6.45652174e-02  1.06304348e-01\n  1.52173913e-03  1.34782609e-02  4.50000000e-02  1.26086957e-02\n  3.91304348e-03  3.56521739e-02  2.28260870e-02  4.39130435e-02\n  6.82608696e-02  2.60869565e-03  2.21739130e-02  2.00652174e-02\n  9.80217391e-02  3.62608696e-02  3.10695652e-01  1.86760870e-01\n  8.23260870e-02  3.43565870e+01  3.78304348e+02  2.24915217e+03\n  7.82608696e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "labels_node1 = np.array([0, 0, 0])  # Node 1 labels\n",
        "labels_node2 = np.array([1, 1, 1])  # Node 2 labels\n",
        "\n",
        "new_point = np.array([4, 4])  # New point to classify\n",
        "k = 3  # Number of neighbors\n",
        "\n",
        "# Local distance calculation at each node\n",
        "distances_node1 = distance.cdist([new_point], data_node1, 'euclidean')[0]\n",
        "distances_node2 = distance.cdist([new_point], data_node2, 'euclidean')[0]\n",
        "\n",
        "# Combine distances and labels\n",
        "all_distances = np.concatenate((distances_node1, distances_node2))\n",
        "all_labels = np.concatenate((labels_node1, labels_node2))\n",
        "\n",
        "# Find the k-nearest neighbors\n",
        "nearest_indices = np.argsort(all_distances)[:k]\n",
        "nearest_labels = all_labels[nearest_indices]\n",
        "\n",
        "# Perform majority voting\n",
        "from collections import Counter\n",
        "label = Counter(nearest_labels).most_common(1)[0][0]\n",
        "\n",
        "print(f\"The new point is classified as: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh8FrBCPcO8M",
        "outputId": "2a3c12e7-b40d-4809-d367-2df15fb0d100"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new point is classified as: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "\n",
        "k = 2  # Number of clusters\n",
        "iterations = 5\n",
        "\n",
        "# Initialize centroids randomly from all data points (here, we're using only node1's data for simplicity)\n",
        "centroids = np.array([[2, 3], [6, 6]])\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    # Local assignment at each node\n",
        "    distances_node1 = np.linalg.norm(data_node1[:, np.newaxis] - centroids, axis=2)\n",
        "    distances_node2 = np.linalg.norm(data_node2[:, np.newaxis] - centroids, axis=2)\n",
        "\n",
        "    cluster_assignments_node1 = np.argmin(distances_node1, axis=1)\n",
        "    cluster_assignments_node2 = np.argmin(distances_node2, axis=1)\n",
        "\n",
        "    # Local centroid update\n",
        "    new_centroids_node1 = np.array([data_node1[cluster_assignments_node1 == i].mean(axis=0) for i in range(k)])\n",
        "    new_centroids_node2 = np.array([data_node2[cluster_assignments_node2 == i].mean(axis=0) for i in range(k)])\n",
        "\n",
        "    # Global centroid update (average of all nodes)\n",
        "    centroids = (new_centroids_node1 + new_centroids_node2) / 2\n",
        "\n",
        "    print(f\"Iteration {iteration+1}: Centroids: {centroids}\")\n",
        "\n",
        "print(f\"Final centroids: {centroids}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Xy8dN51cSfn",
        "outputId": "18e70ac6-80fd-4af8-fd2f-2ee8b0a86b6f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Centroids: [[nan nan]\n",
            " [nan nan]]\n",
            "Iteration 2: Centroids: [[4.         4.66666667]\n",
            " [       nan        nan]]\n",
            "Iteration 3: Centroids: [[       nan        nan]\n",
            " [4.         4.66666667]]\n",
            "Iteration 4: Centroids: [[4.         4.66666667]\n",
            " [       nan        nan]]\n",
            "Iteration 5: Centroids: [[       nan        nan]\n",
            " [4.         4.66666667]]\n",
            "Final centroids: [[       nan        nan]\n",
            " [4.         4.66666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "# Include both class labels in node1's data\n",
        "labels_node1 = np.array([0, 0, 1])  # Node 1 labels\n",
        "\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "labels_node2 = np.array([1, 1, 1])  # Node 2 labels\n",
        "\n",
        "# Initialize the SVM model with a linear kernel\n",
        "model = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Local training at each node\n",
        "model.fit(data_node1, labels_node1)  # Train on Node 1 data\n",
        "model.fit(data_node2, labels_node2)  # Train on Node 2 data (updating the model)\n",
        "\n",
        "# Test the model on a new point\n",
        "new_point = np.array([[4, 4]])\n",
        "predicted_label = model.predict(new_point)\n",
        "\n",
        "print(f\"The new point is classified as: {predicted_label[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "NQNVAR7kdlJx",
        "outputId": "75df6e74-df7e-4ff5-a1d4-d1cca8a0fd55"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The number of classes has to be greater than one; got 1 class",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3d8f9c250b82>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Local training at each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_node1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train on Node 1 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_node2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train on Node 2 data (updating the model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Test the model on a new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         sample_weight = np.asarray(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from collections import Counter\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "labels_node1 = np.array([0, 0, 0])  # Node 1 labels\n",
        "labels_node2 = np.array([1, 1, 1])  # Node 2 labels\n",
        "\n",
        "new_point = np.array([4, 4])  # New point to classify\n",
        "k = 3  # Number of neighbors\n",
        "\n",
        "# Local distance calculation at each node\n",
        "distances_node1 = distance.cdist([new_point], data_node1, 'euclidean')[0]\n",
        "distances_node2 = distance.cdist([new_point], data_node2, 'euclidean')[0]\n",
        "\n",
        "# Combine distances and labels from both nodes\n",
        "all_distances = np.concatenate((distances_node1, distances_node2))\n",
        "all_labels = np.concatenate((labels_node1, labels_node2))\n",
        "\n",
        "# Find the k-nearest neighbors by sorting distances\n",
        "nearest_indices = np.argsort(all_distances)[:k]\n",
        "nearest_labels = all_labels[nearest_indices]\n",
        "\n",
        "# Perform majority voting among the k-nearest neighbors\n",
        "label = Counter(nearest_labels).most_common(1)[0][0]\n",
        "\n",
        "print(f\"The new point is classified as: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqDcdvMbeZpM",
        "outputId": "486ba38d-f219-4c13-e5a1-334876a63cd8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new point is classified as: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "\n",
        "k = 2  # Number of clusters\n",
        "iterations = 5  # Number of iterations for convergence\n",
        "\n",
        "# Initialize centroids randomly from all data points (using node1's data for simplicity)\n",
        "centroids = np.array([[2, 3], [6, 6]])\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    # Local assignment of data points to the nearest centroid at each node\n",
        "    distances_node1 = np.linalg.norm(data_node1[:, np.newaxis] - centroids, axis=2)\n",
        "    distances_node2 = np.linalg.norm(data_node2[:, np.newaxis] - centroids, axis=2)\n",
        "\n",
        "    # Determine the nearest centroid for each point\n",
        "    cluster_assignments_node1 = np.argmin(distances_node1, axis=1)\n",
        "    cluster_assignments_node2 = np.argmin(distances_node2, axis=1)\n",
        "\n",
        "    # Local centroid update with check for empty clusters\n",
        "    new_centroids_node1 = np.array([\n",
        "        data_node1[cluster_assignments_node1 == i].mean(axis=0) if np.any(cluster_assignments_node1 == i) else centroids[i]\n",
        "        for i in range(k)\n",
        "    ])\n",
        "    new_centroids_node2 = np.array([\n",
        "        data_node2[cluster_assignments_node2 == i].mean(axis=0) if np.any(cluster_assignments_node2 == i) else centroids[i]\n",
        "        for i in range(k)\n",
        "    ])\n",
        "\n",
        "    # Global centroid update by averaging centroids from all nodes\n",
        "    centroids = (new_centroids_node1 + new_centroids_node2) / 2\n",
        "\n",
        "    print(f\"Iteration {iteration+1}: Centroids: {centroids}\")\n",
        "\n",
        "# Final centroids after all iterations\n",
        "print(f\"Final centroids: {centroids}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgMHXawaecBd",
        "outputId": "86098bdf-ad1e-4907-fed5-a0816ad2d791"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Centroids: [[2.         3.        ]\n",
            " [6.         6.16666667]]\n",
            "Iteration 2: Centroids: [[2.   3.  ]\n",
            " [6.   6.25]]\n",
            "Iteration 3: Centroids: [[2.         3.        ]\n",
            " [6.         6.29166667]]\n",
            "Iteration 4: Centroids: [[2.     3.    ]\n",
            " [6.     6.3125]]\n",
            "Iteration 5: Centroids: [[2.         3.        ]\n",
            " [6.         6.32291667]]\n",
            "Final centroids: [[2.         3.        ]\n",
            " [6.         6.32291667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "labels_node1 = np.array([0, 0, 0])  # Node 1 labels\n",
        "\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "labels_node2 = np.array([1, 1, 1])  # Node 2 labels\n",
        "\n",
        "# Initialize the SVM model with a linear kernel\n",
        "model = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Local training at each node\n",
        "model.fit(data_node1, labels_node1)  # Train on Node 1 data\n",
        "model.fit(data_node2, labels_node2)  # Train on Node 2 data (updating the model)\n",
        "\n",
        "# Test the model on a new point\n",
        "new_point = np.array([[4, 4]])\n",
        "predicted_label = model.predict(new_point)\n",
        "\n",
        "print(f\"The new point is classified as: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "PlgeOd6ned6O",
        "outputId": "6fc3720e-786b-4b76-9696-93dfc9b1a2f2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The number of classes has to be greater than one; got 1 class",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-8d9919611a4b>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Local training at each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_node1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train on Node 1 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_node2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train on Node 2 data (updating the model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         sample_weight = np.asarray(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "from collections import Counter\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "labels_node1 = np.array([0, 0, 0])  # Node 1 labels\n",
        "labels_node2 = np.array([1, 1, 1])  # Node 2 labels\n",
        "\n",
        "new_point = np.array([4, 4])  # New point to classify\n",
        "k = 3  # Number of neighbors\n",
        "\n",
        "# Local distance calculation at each node\n",
        "distances_node1 = distance.cdist([new_point], data_node1, 'euclidean')[0]\n",
        "distances_node2 = distance.cdist([new_point], data_node2, 'euclidean')[0]\n",
        "\n",
        "# Combine distances and labels from both nodes\n",
        "all_distances = np.concatenate((distances_node1, distances_node2))\n",
        "all_labels = np.concatenate((labels_node1, labels_node2))\n",
        "\n",
        "# Find the k-nearest neighbors by sorting distances\n",
        "nearest_indices = np.argsort(all_distances)[:k]\n",
        "nearest_labels = all_labels[nearest_indices]\n",
        "\n",
        "# Perform majority voting among the k-nearest neighbors\n",
        "label = Counter(nearest_labels).most_common(1)[0][0]\n",
        "\n",
        "print(f\"The new point is classified as: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MNxOnnHff4m",
        "outputId": "eade39c1-f3a7-4e39-e10f-3fb612e6bc45"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new point is classified as: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "\n",
        "k = 2  # Number of clusters\n",
        "iterations = 5  # Number of iterations for convergence\n",
        "\n",
        "# Initialize centroids randomly from all data points (using node1's data for simplicity)\n",
        "centroids = np.array([[2, 3], [6, 6]])\n",
        "\n",
        "for iteration in range(iterations):\n",
        "    # Local assignment of data points to the nearest centroid at each node\n",
        "    distances_node1 = np.linalg.norm(data_node1[:, np.newaxis] - centroids, axis=2)\n",
        "    distances_node2 = np.linalg.norm(data_node2[:, np.newaxis] - centroids, axis=2)\n",
        "\n",
        "    # Determine the nearest centroid for each point\n",
        "    cluster_assignments_node1 = np.argmin(distances_node1, axis=1)\n",
        "    cluster_assignments_node2 = np.argmin(distances_node2, axis=1)\n",
        "\n",
        "    # Local centroid update with check for empty clusters\n",
        "    new_centroids_node1 = np.array([\n",
        "        data_node1[cluster_assignments_node1 == i].mean(axis=0) if np.any(cluster_assignments_node1 == i) else centroids[i]\n",
        "        for i in range(k)\n",
        "    ])\n",
        "    new_centroids_node2 = np.array([\n",
        "        data_node2[cluster_assignments_node2 == i].mean(axis=0) if np.any(cluster_assignments_node2 == i) else centroids[i]\n",
        "        for i in range(k)\n",
        "    ])\n",
        "\n",
        "    # Global centroid update by averaging centroids from all nodes\n",
        "    centroids = (new_centroids_node1 + new_centroids_node2) / 2\n",
        "\n",
        "    print(f\"Iteration {iteration+1}: Centroids: {centroids}\")\n",
        "\n",
        "# Final centroids after all iterations\n",
        "print(f\"Final centroids: {centroids}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlziFTuNfgja",
        "outputId": "d6b25ca3-3520-4f69-e355-ba3b1d212d87"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Centroids: [[2.         3.        ]\n",
            " [6.         6.16666667]]\n",
            "Iteration 2: Centroids: [[2.   3.  ]\n",
            " [6.   6.25]]\n",
            "Iteration 3: Centroids: [[2.         3.        ]\n",
            " [6.         6.29166667]]\n",
            "Iteration 4: Centroids: [[2.     3.    ]\n",
            " [6.     6.3125]]\n",
            "Iteration 5: Centroids: [[2.         3.        ]\n",
            " [6.         6.32291667]]\n",
            "Final centroids: [[2.         3.        ]\n",
            " [6.         6.32291667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Simulated distributed data\n",
        "data_node1 = np.array([[1, 2], [2, 3], [3, 4]])  # Node 1 data\n",
        "labels_node1 = np.array([0, 0, 0])  # Node 1 labels\n",
        "\n",
        "data_node2 = np.array([[5, 5], [6, 6], [7, 8]])  # Node 2 data\n",
        "labels_node2 = np.array([1, 1, 1])  # Node 2 labels\n",
        "\n",
        "# Initialize the SVM model with a linear kernel\n",
        "model = SVC(kernel='linear', C=1.0)\n",
        "\n",
        "# Local training at each node\n",
        "model.fit(data_node1, labels_node1)  # Train on Node 1 data\n",
        "model.fit(data_node2, labels_node2)  # Train on Node 2 data (updating the model)\n",
        "\n",
        "# Test the model on a new point\n",
        "new_point = np.array([[4, 4]])\n",
        "predicted_label = model.predict(new_point)\n",
        "\n",
        "print(f\"The new point is classified as: {predicted_label[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "-Q37v3AWfkOI",
        "outputId": "c93912ee-9fc0-4980-ab37-ee1b6057c821"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The number of classes has to be greater than one; got 1 class",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8d9919611a4b>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Local training at each node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_node1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train on Node 1 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_node2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train on Node 2 data (updating the model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                 )\n\u001b[1;32m   1151\u001b[0m             ):\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m             )\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         sample_weight = np.asarray(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    745\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    748\u001b[0m                 \u001b[0;34m\"The number of classes has to be greater than one; got %d class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m                 \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
          ]
        }
      ]
    }
  ]
}